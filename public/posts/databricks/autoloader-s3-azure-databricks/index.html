<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Using Auto Loader on Azure Databricks with AWS S3 | Ust Does Tech</title>
<meta name="keywords" content="">
<meta name="description" content="All the necessary steps to make Auto Loader on Azure Databricks work with data in an AWS S3 Bucket">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.6da9a63d25a9608bca2f7f907a030e887a7dd3c3f3918e4cc113129361414bda.css" integrity="sha256-bammPSWpYIvKL3&#43;QegMOiHp908PzkY5MwRMSk2FBS9o=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/">
  <meta property="og:site_name" content="Ust Does Tech">
  <meta property="og:title" content="Using Auto Loader on Azure Databricks with AWS S3">
  <meta property="og:description" content="All the necessary steps to make Auto Loader on Azure Databricks work with data in an AWS S3 Bucket">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-10-18T20:29:25+06:00">
    <meta property="article:modified_time" content="2021-10-18T20:29:25+06:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Using Auto Loader on Azure Databricks with AWS S3">
<meta name="twitter:description" content="All the necessary steps to make Auto Loader on Azure Databricks work with data in an AWS S3 Bucket">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Databricks",
      "item": "http://localhost:1313/posts/databricks/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Using Auto Loader on Azure Databricks with AWS S3",
      "item": "http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Using Auto Loader on Azure Databricks with AWS S3",
  "name": "Using Auto Loader on Azure Databricks with AWS S3",
  "description": "All the necessary steps to make Auto Loader on Azure Databricks work with data in an AWS S3 Bucket",
  "keywords": [
    
  ],
  "articleBody": "Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we’re less dependent upon additional systems to provide that “what did we last load” context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn’t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn’t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we’ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn’t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there’s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it’s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\"{secretName}\") return(secret.value) keyVaultName = os.environ[\"KeyVaultName\"] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\"https://{keyVaultName}.vault.azure.net/\", credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \"AWSAccessKey\") secretAccessKey = retrieveSecret(keyVaultClient, \"AWSSecretKey\") Getting Temporary Credentials Now that we’ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ['AWS_ACCESS_KEY_ID'] = accessKeyId os.environ['AWS_SECRET_ACCESS_KEY'] = secretAccessKey botoConfig = Config( region_name = 'eu-west-2', signature_version = 'v4', retries = { 'max_attempts': 10, 'mode': 'standard' } ) client = boto3.client('sts', config = botoConfig) response = client.assume_role( RoleArn='arn:aws:iam::1234567890:role/role_name', RoleSessionName='AzureFunctionRefresh', DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ 'Credentials': { 'AccessKeyId': 'string', 'SecretAccessKey': 'string', 'SessionToken': 'string', 'Expiration': datetime(2015, 1, 1) }, 'AssumedRoleUser': { 'AssumedRoleId': 'string', 'Arn': 'string' }, 'PackedPolicySize': 123, 'SourceIdentity': 'string' } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response['Credentials'] tempAccessKeyId = credResponse['AccessKeyId'] tempSecretAccessKey = credResponse['SecretAccessKey'] tempSessionToken = credResponse['SessionToken'] updateSecret(keyVaultClient, \"TempAccessKeyId\", tempAccessKeyId) updateSecret(keyVaultClient, \"TempSecretAccessKey\", tempSecretAccessKey) updateSecret(keyVaultClient, \"TempSessionToken\", tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\"fs.s3a.credentialsType\", \"AssumeRole\") spark.conf.set(\"fs.s3a.stsAssumeRole.arn\", \"arn:aws:iam::123456789:role/role_name\") spark.conf.set(\"fs.s3a.acl.default\", \"BucketOwnerFullControl\") AccessKey = dbutils.secrets.get(\"ScopeName\", key = \"TempAccessKeyId\") SecretKey = dbutils.secrets.get(\"ScopeName\", key = \"TempSecretAccessKey\") SessionToken = dbutils.secrets.get(\"ScopeName\", key = \"TempSessionToken\") sc._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\") sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AccessKeyId) sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", SecretKey) sc._jsc.hadoopConfiguration().set(\"fs.s3a.session.token\", SessionToken) Note how we’re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn’t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\"cloudFiles\") .option(\"cloudFiles.format\", \"json\") .schema(definedSchema) .load(\"s3a://bucket/directory/\") ) And writing out is just as easy.\n(df.writeStream.format(\"delta\") .option(\"checkpointLocation\", \"/mnt/lake/directory/_checkpoint\") .trigger(once=True) .start(\"/mnt/lake/directory/\") ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn’t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n",
  "wordCount" : "982",
  "inLanguage": "en",
  "datePublished": "2021-10-18T20:29:25+06:00",
  "dateModified": "2021-10-18T20:29:25+06:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Ust Does Tech",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Ust Does Tech (Alt + H)">Ust Does Tech</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Using Auto Loader on Azure Databricks with AWS S3
    </h1>
    <div class="post-description">
      All the necessary steps to make Auto Loader on Azure Databricks work with data in an AWS S3 Bucket
    </div>
    <div class="post-meta"><span title='2021-10-18 20:29:25 +0600 +0600'>October 18, 2021</span>

</div>
  </header> 
  <div class="post-content"><h1 id="problem">Problem<a hidden class="anchor" aria-hidden="true" href="#problem">#</a></h1>
<p>Recently on a client project, we wanted to use the <a href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader">Auto Loader</a> functionality in Databricks to easily consume from <a href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3">AWS S3</a> into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from <a href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2">Azure Storage Accounts</a> and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we&rsquo;re less dependent upon additional systems to provide that &ldquo;what did we last load&rdquo; context.</p>
<p>We followed the steps on the Microsoft Docs to <a href="https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3">load files in from AWS S3 using Auto Loader</a> but we were getting an error message that couldn&rsquo;t be easily resolved in the Azure instance of Databricks:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-typescript" data-lang="typescript"><span style="display:flex;"><span><span style="color:#a6e22e">shaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException</span>: 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">Instantiate</span> <span style="color:#a6e22e">shaded</span>.<span style="color:#a6e22e">databricks</span>.<span style="color:#a6e22e">org</span>.<span style="color:#a6e22e">apache</span>.<span style="color:#a6e22e">hadoop</span>.<span style="color:#a6e22e">fs</span>.<span style="color:#a6e22e">s3a</span>.<span style="color:#a6e22e">auth</span>.<span style="color:#a6e22e">AssumedRoleCredentialProvider</span> <span style="color:#a6e22e">on</span> : 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">com.amazonaws.AmazonClientException</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">No</span> <span style="color:#a6e22e">AWS</span> <span style="color:#a6e22e">Credentials</span> <span style="color:#a6e22e">provided</span> <span style="color:#a6e22e">by</span> <span style="color:#a6e22e">InstanceProfileCredentialsProvider</span> : 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">com.amazonaws.SdkClientException</span><span style="color:#f92672">:</span> 
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">The</span> <span style="color:#a6e22e">requested</span> <span style="color:#a6e22e">metadata</span> <span style="color:#66d9ef">is</span> <span style="color:#a6e22e">not</span> <span style="color:#a6e22e">found</span> <span style="color:#a6e22e">at</span> <span style="color:#a6e22e">http</span><span style="color:#f92672">:</span><span style="color:#75715e">//169.254.169.254/latest/meta-data/iam/security-credentials/: 
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#a6e22e">No</span> <span style="color:#a6e22e">AWS</span> <span style="color:#a6e22e">Credentials</span> <span style="color:#a6e22e">provided</span> <span style="color:#a6e22e">by</span> <span style="color:#a6e22e">InstanceProfileCredentialsProvider</span> : 
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">com.amazonaws.SdkClientException</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">The</span> <span style="color:#a6e22e">requested</span> <span style="color:#a6e22e">metadata</span> <span style="color:#66d9ef">is</span> <span style="color:#a6e22e">not</span> <span style="color:#a6e22e">found</span> <span style="color:#a6e22e">at</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">http</span><span style="color:#f92672">:</span><span style="color:#75715e">//169.254.169.254/latest/meta-data/iam/security-credentials/
</span></span></span></code></pre></div><p>Azure doesn&rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.</p>
<h1 id="solution">Solution<a hidden class="anchor" aria-hidden="true" href="#solution">#</a></h1>
<p>We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html">temporary credentials</a>. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.</p>
<h2 id="temporary-credential-generation">Temporary Credential Generation<a hidden class="anchor" aria-hidden="true" href="#temporary-credential-generation">#</a></h2>
<p>Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we&rsquo;ll want to store the temporary credentials in <a href="https://docs.microsoft.com/en-us/azure/key-vault/general/overview">Azure Key Vault</a> - so that they are secured.</p>
<p>For this, we looked towards <a href="https://docs.microsoft.com/en-us/azure/azure-functions/functions-overview">Azure Functions</a> because, if Auto Loader wouldn&rsquo;t work with them then we could use <a href="https://docs.microsoft.com/en-us/azure/data-factory/introduction">Azure Data Factory</a> as a fallback option to load data into Azure.</p>
<p>Following the steps laid out in the Microsoft documentation, we created our <a href="https://docs.microsoft.com/en-us/azure/azure-functions/create-first-function-vs-code-python">Function in Python</a>.</p>
<h3 id="python-libraries">Python Libraries<a hidden class="anchor" aria-hidden="true" href="#python-libraries">#</a></h3>
<p>For our function to work, we needed the <a href="https://aws.amazon.com/sdk-for-python/">boto3 library for AWS</a> and a whole host of Azure libraries to connect to Key Vault securely.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> boto3
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> azure.keyvault.secrets <span style="color:#f92672">import</span> SecretClient
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> azure.identity <span style="color:#f92672">import</span> DefaultAzureCredential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> botocore.config <span style="color:#f92672">import</span> Config
</span></span></code></pre></div><h3 id="functional-steps">Functional Steps<a hidden class="anchor" aria-hidden="true" href="#functional-steps">#</a></h3>
<h4 id="connecting-to-key-vault">Connecting to Key Vault<a hidden class="anchor" aria-hidden="true" href="#connecting-to-key-vault">#</a></h4>
<p>Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there&rsquo;s a great blog post by <a href="https://twitter.com/DKrzyczkowski">Daniel Krzyczkowski</a> which sets out the steps for <a href="https://daniel-krzyczkowski.github.io/Integrate-Key-Vault-Secrets-With-Azure-Functions/">configuring your Function App</a>.</p>
<p>We added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:</p>
<ul>
<li>AZURE_CLIENT_ID</li>
<li>AZURE_CLIENT_SECRET</li>
<li>AZURE_TENANT_ID</li>
</ul>
<p>A circular reference to Key Vault might sound counter-intuitive, but it&rsquo;s the only way I could see that would allow the app to update secrets.</p>
<p>Bringing back secrets from Key Vault required the following code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">retrieveSecret</span>(client, secretName):
</span></span><span style="display:flex;"><span>    secret <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>get_secret(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>secretName<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span>(secret<span style="color:#f92672">.</span>value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>keyVaultName <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;KeyVaultName&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>keyVaultCredential <span style="color:#f92672">=</span> DefaultAzureCredential()
</span></span><span style="display:flex;"><span>keyVaultClient <span style="color:#f92672">=</span> SecretClient(vault_url<span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;https://</span><span style="color:#e6db74">{</span>keyVaultName<span style="color:#e6db74">}</span><span style="color:#e6db74">.vault.azure.net/&#34;</span>, credential<span style="color:#f92672">=</span>keyVaultCredential)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>accessKeyId <span style="color:#f92672">=</span> retrieveSecret(keyVaultClient, <span style="color:#e6db74">&#34;AWSAccessKey&#34;</span>)
</span></span><span style="display:flex;"><span>secretAccessKey <span style="color:#f92672">=</span> retrieveSecret(keyVaultClient, <span style="color:#e6db74">&#34;AWSSecretKey&#34;</span>)
</span></span></code></pre></div><h4 id="getting-temporary-credentials">Getting Temporary Credentials<a hidden class="anchor" aria-hidden="true" href="#getting-temporary-credentials">#</a></h4>
<p>Now that we&rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sts.html#STS.Client.assume_role">assume role</a> functionality.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;AWS_ACCESS_KEY_ID&#39;</span>] <span style="color:#f92672">=</span> accessKeyId 
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#39;AWS_SECRET_ACCESS_KEY&#39;</span>] <span style="color:#f92672">=</span> secretAccessKey 
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>botoConfig <span style="color:#f92672">=</span> Config(
</span></span><span style="display:flex;"><span>    region_name <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;eu-west-2&#39;</span>,
</span></span><span style="display:flex;"><span>    signature_version <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;v4&#39;</span>,
</span></span><span style="display:flex;"><span>    retries <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;max_attempts&#39;</span>: <span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;mode&#39;</span>: <span style="color:#e6db74">&#39;standard&#39;</span>
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>client <span style="color:#f92672">=</span> boto3<span style="color:#f92672">.</span>client(<span style="color:#e6db74">&#39;sts&#39;</span>, config <span style="color:#f92672">=</span> botoConfig)
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>assume_role(
</span></span><span style="display:flex;"><span>    RoleArn<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;arn:aws:iam::1234567890:role/role_name&#39;</span>,
</span></span><span style="display:flex;"><span>    RoleSessionName<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;AzureFunctionRefresh&#39;</span>,
</span></span><span style="display:flex;"><span>    DurationSeconds<span style="color:#f92672">=</span><span style="color:#ae81ff">3600</span>
</span></span><span style="display:flex;"><span>)   
</span></span></code></pre></div><p>This returned our temporary credentials as a nice JSON object:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">&#39;Credentials&#39;:</span> <span style="color:#960050;background-color:#1e0010">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">&#39;AccessKeyId&#39;:</span> <span style="color:#960050;background-color:#1e0010">&#39;string&#39;,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">&#39;SecretAccessKey&#39;:</span> <span style="color:#960050;background-color:#1e0010">&#39;string&#39;,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">&#39;SessionToken&#39;:</span> <span style="color:#960050;background-color:#1e0010">&#39;string&#39;,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">&#39;Expiration&#39;:</span> <span style="color:#960050;background-color:#1e0010">datetime(2015,</span> <span style="color:#960050;background-color:#1e0010">1,</span> <span style="color:#960050;background-color:#1e0010">1)</span>
</span></span><span style="display:flex;"><span>    }<span style="color:#960050;background-color:#1e0010">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">&#39;AssumedRoleUser&#39;:</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">&#39;AssumedRoleId&#39;:</span> <span style="color:#960050;background-color:#1e0010">&#39;string&#39;,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#960050;background-color:#1e0010">&#39;Arn&#39;:</span> <span style="color:#960050;background-color:#1e0010">&#39;string&#39;</span>
</span></span><span style="display:flex;"><span>    }<span style="color:#960050;background-color:#1e0010">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">&#39;PackedPolicySize&#39;:</span> <span style="color:#ae81ff">123</span><span style="color:#960050;background-color:#1e0010">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#960050;background-color:#1e0010">&#39;SourceIdentity&#39;:</span> <span style="color:#960050;background-color:#1e0010">&#39;string&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">}</span>
</span></span></code></pre></div><p>We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">updateSecret</span>(client, secretName, secretValue):
</span></span><span style="display:flex;"><span>    updated_secret <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>set_secret(secretName, secretValue)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>credResponse <span style="color:#f92672">=</span> response[<span style="color:#e6db74">&#39;Credentials&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tempAccessKeyId <span style="color:#f92672">=</span> credResponse[<span style="color:#e6db74">&#39;AccessKeyId&#39;</span>]
</span></span><span style="display:flex;"><span>tempSecretAccessKey <span style="color:#f92672">=</span> credResponse[<span style="color:#e6db74">&#39;SecretAccessKey&#39;</span>]
</span></span><span style="display:flex;"><span>tempSessionToken <span style="color:#f92672">=</span> credResponse[<span style="color:#e6db74">&#39;SessionToken&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>updateSecret(keyVaultClient, <span style="color:#e6db74">&#34;TempAccessKeyId&#34;</span>, tempAccessKeyId)
</span></span><span style="display:flex;"><span>updateSecret(keyVaultClient, <span style="color:#e6db74">&#34;TempSecretAccessKey&#34;</span>, tempSecretAccessKey)
</span></span><span style="display:flex;"><span>updateSecret(keyVaultClient, <span style="color:#e6db74">&#34;TempSessionToken&#34;</span>, tempSessionToken)
</span></span></code></pre></div><h4 id="getting-databricks-to-work">Getting Databricks to work<a hidden class="anchor" aria-hidden="true" href="#getting-databricks-to-work">#</a></h4>
<p>Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.</p>
<p>Our first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.credentialsType&#34;</span>, <span style="color:#e6db74">&#34;AssumeRole&#34;</span>)
</span></span><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.stsAssumeRole.arn&#34;</span>, <span style="color:#e6db74">&#34;arn:aws:iam::123456789:role/role_name&#34;</span>)
</span></span><span style="display:flex;"><span>spark<span style="color:#f92672">.</span>conf<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.acl.default&#34;</span>, <span style="color:#e6db74">&#34;BucketOwnerFullControl&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>AccessKey <span style="color:#f92672">=</span> dbutils<span style="color:#f92672">.</span>secrets<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;ScopeName&#34;</span>, key <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;TempAccessKeyId&#34;</span>)
</span></span><span style="display:flex;"><span>SecretKey <span style="color:#f92672">=</span> dbutils<span style="color:#f92672">.</span>secrets<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;ScopeName&#34;</span>, key <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;TempSecretAccessKey&#34;</span>)
</span></span><span style="display:flex;"><span>SessionToken <span style="color:#f92672">=</span> dbutils<span style="color:#f92672">.</span>secrets<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#34;ScopeName&#34;</span>, key <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;TempSessionToken&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>_jsc<span style="color:#f92672">.</span>hadoopConfiguration()<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.aws.credentials.provider&#34;</span>, <span style="color:#e6db74">&#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider&#34;</span>)
</span></span><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>_jsc<span style="color:#f92672">.</span>hadoopConfiguration()<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.access.key&#34;</span>, AccessKeyId)
</span></span><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>_jsc<span style="color:#f92672">.</span>hadoopConfiguration()<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.secret.key&#34;</span>, SecretKey)
</span></span><span style="display:flex;"><span>sc<span style="color:#f92672">.</span>_jsc<span style="color:#f92672">.</span>hadoopConfiguration()<span style="color:#f92672">.</span>set(<span style="color:#e6db74">&#34;fs.s3a.session.token&#34;</span>, SessionToken)
</span></span></code></pre></div><p>Note how we&rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.</p>
<p>We can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn&rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#f92672">=</span> (spark<span style="color:#f92672">.</span>readStream<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;cloudFiles&#34;</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;cloudFiles.format&#34;</span>, <span style="color:#e6db74">&#34;json&#34;</span>)
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">.</span>schema(definedSchema)
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;s3a://bucket/directory/&#34;</span>)
</span></span><span style="display:flex;"><span>     )
</span></span></code></pre></div><p>And writing out is just as easy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(df<span style="color:#f92672">.</span>writeStream<span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;delta&#34;</span>) 
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">.</span>option(<span style="color:#e6db74">&#34;checkpointLocation&#34;</span>, <span style="color:#e6db74">&#34;/mnt/lake/directory/_checkpoint&#34;</span>) 
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">.</span>trigger(once<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">.</span>start(<span style="color:#e6db74">&#34;/mnt/lake/directory/&#34;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.</p>
<p>This is one way of getting it to work. If there wasn&rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.</p>
<p>Thanks for reading!</p>
<p>UPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.</p>
<script src="https://gist.github.com/UstDoesTech/172525c34c4bea5650e1338e77c5bf3a.js"></script>



  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Ust Does Tech</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
