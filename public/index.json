[{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nTime to break stuff Remember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy that idea.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (c) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nTime to break stuff Remember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy that idea.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nTime to break stuff Remember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy that idea.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;).\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nTime to break stuff Remember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy that idea.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it?).\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nTime to break stuff Remember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy that idea.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nTime to break laws Remember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“In a dynamic number system where values are defined relative to a state function over time, every number loses its fixed identity under sufficient transformation of context.”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)=N(t0​) and may even become undefined ”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)=N(t0​) and may even become undefined ”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined ”\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples Forecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians shouting at me for destroying their world view. I\u0026rsquo;m not sorry. You\u0026rsquo;ll get over it.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples These are examples in which I have applied the idea of numerical impermanence to my work. They are not exhaustive, but they are a good starting point for thinking about how we can apply this concept to our systems.\nForecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. Churn Modelling: When we model churn, we are trying to predict which customers are likely to leave. But if the context is constantly changing, how can we trust our models? The idea of numerical impermanence suggests that we need to apply context to our models for them to be meaningful. ","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians shouting at me for destroying their world view. I\u0026rsquo;m not sorry. You\u0026rsquo;ll get over it.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples These are examples in which I have applied the idea of numerical impermanence to my work. They are not exhaustive, but they are a good starting point for thinking about how we can apply this concept to our systems.\nForecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. Churn Modelling: When we model churn, we are trying to predict which customers are likely to leave. Applying context to our models can help us understand why customers are leaving and how we can prevent it. Elasticity: When we model elasticity, we are trying to understand how changes in price will affect demand. Contexts change\u0026hellip; ALL THE TIME. I could go on, but I think you get the point. Things change. Context is king - it explains and drives the change. Embrace the change. Embrace impermanence.\nFor some disciplines, like economics, the idea of numerical impermanence is as old as time, you might as well call it inflation. Numerical impermanence, as formalised, is a novel application of the old idea of impermanence to the oldest science in the world.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians shouting at me for destroying their world view. I\u0026rsquo;m not sorry. You\u0026rsquo;ll get over it.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples These are examples in which I have applied the idea of numerical impermanence to my work. They are not exhaustive, but they are a good starting point for thinking about how we can apply this concept to our systems.\nForecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. Churn Modelling: When we model churn, we are trying to predict which customers are likely to leave. Applying context to our models can help us understand why customers are leaving and how we can prevent it. Elasticity: When we model elasticity, we are trying to understand how changes in price will affect demand. Contexts change\u0026hellip; ALL THE TIME. I could go on, but I think you get the point. Things change. Context is king - it explains and drives the change. Embrace the change. Embrace impermanence.\nThe Takeaway For some disciplines, like economics, the idea of numerical impermanence is as old as time, you might as well call it inflation. Numerical impermanence, as formalised, is a novel application of the old idea of impermanence to the oldest science in the world.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians shouting at me for destroying their world view. I\u0026rsquo;m not sorry. You\u0026rsquo;ll get over it.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples These are examples in which I have applied the idea of numerical impermanence to my work. They are not exhaustive, but they are a good starting point for thinking about how we can apply this concept to our systems.\nForecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. Churn Modelling: When we model churn, we are trying to predict which customers are likely to leave. Applying context to our models can help us understand why customers are leaving and how we can prevent it. Elasticity: When we model elasticity, we are trying to understand how changes in price will affect demand. Contexts change\u0026hellip; ALL THE TIME. I could go on, but I think you get the point. Things change. Context is king - it explains and drives the change.\nThe Takeaway For some disciplines, like economics, the idea of numerical impermanence is as old as time, you might as well call it inflation. Numerical impermanence, as formalised, is a novel application of the old idea of impermanence to the oldest science in the world. For mathematicians, I hope you\u0026rsquo;re not angry at me for breaking your world view. I hope you can see the value in embracing impermanence and using it to our advantage.\nFor the rest of us, I hope you can see the value in applying this concept to our systems and using it to help us navigate the chaos of life. Embrace impermanence. Embrace change. And remember that nothing is permanent - not even numbers.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea of that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t0), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t0​) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians shouting at me for destroying their world view. I\u0026rsquo;m not sorry. You\u0026rsquo;ll get over it.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples These are examples in which I have applied the idea of numerical impermanence to my work. They are not exhaustive, but they are a good starting point for thinking about how we can apply this concept to our systems.\nForecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. Churn Modelling: When we model churn, we are trying to predict which customers are likely to leave. Applying context to our models can help us understand why customers are leaving and how we can prevent it. Elasticity: When we model elasticity, we are trying to understand how changes in price will affect demand. Contexts change\u0026hellip; ALL THE TIME. I could go on, but I think you get the point. Things change. Context is king - it explains and drives the change.\nThe Takeaway For some disciplines, like economics, the idea of numerical impermanence is as old as time, you might as well call it inflation. Numerical impermanence, as formalised, is a novel application of the old idea of impermanence to the oldest science in the world. For mathematicians, I hope you\u0026rsquo;re not angry at me for breaking your world view. I hope you can see the value in embracing impermanence and using it to our advantage.\nFor the rest of us, I hope you can see the value in applying this concept to our systems and using it to help us navigate the chaos of life. Embrace impermanence. Embrace change. And remember that nothing is permanent - not even numbers.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"In a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of numerical impermanence. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\nSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\nWhat is Numerical Impermanence? Before getting into the details, let\u0026rsquo;s start with impermanence.\nImpermanence Impermanence is the idea that everything is in a constant state of flux and change. Nothing is permanent, and everything is subject to change. This concept is often associated with Buddhism - all temporal things, whether material or mental, are compounded objects in a continuous change of condition, subject to decline and destruction. All physical and mental events are not metaphysically real. They are not constant or permanent; they come into being and dissolve. Basically, don\u0026rsquo;t get too attached to anything, because it will change.\nBreaking the Law This has nothing to do with Judas Priest.\nRemember when I said that impermanence helps me stay grounded? And that mathematics is a grounded discipline? Well\u0026hellip; Let\u0026rsquo;s destroy the idea that it\u0026rsquo;s a grounded discipline.\nMathematics is a discipline that is often seen as rigid and unchanging. But the reality is that mathematics is constantly evolving and changing. The concept of numerical impermanence is about embracing this change and using it to our advantage. However, the one thing that is going to undermine the core tenets of mathematics, the idea that numbers are fixed and unchanging, is the idea that numbers are not fixed and unchanging. Numbers are not permanent. They are not constant. They are not immutable. They are not even real (with some branches of mathematics going \u0026ldquo;well, duh\u0026rdquo;).\nStill with me?\nGood. Time to get a bit more technical.\nThe Setup Core tenets of mathematics state that numbers are fixed. 1 = 1.\nNumbers out in the wild are not fixed - they are shaped by the context in which they are used. A number is always interpreted in context: time, environment, relationship, purpose, and so on, all shape the meaning of a number. This observation, in addition to the embrace of impermanence, has lead to the idea of numerical impermanence - numbers don\u0026rsquo;t have fixed identities when the context changes enough. This is a radical departure from the core tenets of mathematics, but it is one that I believe is necessary in order to embrace the impermanence of life and the world around us.\nThe Theorem (in English) The theorem of numerical impermanence states that:\n“For any initial value N(t₀), there exists a transformation of C(t) such that limt→∞​N(t)≠N(t₀) and may even become undefined”\nOk - that was technically English, but still a bit technical. Let\u0026rsquo;s break it down.\nLet\u0026rsquo;s introduce a new number: the Temporal Number N(t).\nInstead of being that it\u0026rsquo;s a fixed value, like 1, 2, or 3, the Temporal Number N(t) changes over time. Its value at any one time is dependent on two things:\nTime (t) - where we are on a timeline Context (C) - the circumstances surrounding the number We define the Temporal Number N(t) as:\nN(t) = f(t, C(t)) Here, C(t) is itself constantly changing — like a river flowing and altering the meaning of things as it goes (shout out to the Heraclitus fans).\nThe big claim If you pick a starting point in time (say, t₀) and look at the value of the Temporal Number at that time, as time goes on (t → infinity), the number will almost certainly drift away from its original value. Eventually, it might even lose meaning completely (it could become \u0026ldquo;undefined\u0026rdquo;). In seeking meaning, we could end up losing it completely (is the risk worth it? Time will tell).\nThe implications The first implication is a load of angry mathematicians shouting at me for destroying their world view. I\u0026rsquo;m not sorry. You\u0026rsquo;ll get over it.\nThe second set of implications is how we can apply this to our systems. The idea of numerical impermanence is not just a theoretical concept, but one that has practical implications for how we design and use systems.\nSome examples These are examples in which I have applied the idea of numerical impermanence to my work. They are not exhaustive, but they are a good starting point for thinking about how we can apply this concept to our systems.\nForecasting: When we forecast, we are trying to predict the future based on past data. But if the data is constantly changing, how can we trust our forecasts? The idea of numerical impermanence suggests that we need to apply context to our forecasts for them to be meaningful. Churn Modelling: When we model churn, we are trying to predict which customers are likely to leave. Applying context to our models can help us understand why customers are leaving and how we can prevent it. Elasticity: When we model elasticity, we are trying to understand how changes in price will affect demand. Contexts change\u0026hellip; ALL THE TIME. I could go on, but I think you get the point. Things change. Context is king - it explains and drives the change.\nThe Takeaway For some disciplines, like economics, the idea of numerical impermanence is as old as time, you might as well call it inflation. Numerical impermanence, as formalised, is a novel application of the old idea of impermanence to the oldest science in the world. For mathematicians, I hope you\u0026rsquo;re not angry at me for breaking your world view. I hope you can see the value in embracing impermanence and using it to our advantage.\nUltimately, the theorem of numerical impermanence isn\u0026rsquo;t about rejecting mathematics. It\u0026rsquo;s about embracing the living, breathing nature of numbers we see every day in the world around us. In a changing world, the most powerful numbers are the ones we understand in context.\nFor the rest of us, I hope you can see the value in applying this concept to our systems and using it to help us navigate the chaos of life. Embrace impermanence. Embrace change. And remember that nothing is permanent - not even numbers.\nIn future posts we\u0026rsquo;ll explore the theorem in more detail as well as look at some of the practical applications.\n","permalink":"http://localhost:1313/posts/strategy/numerical-impermanence-intro/","summary":"\u003cp\u003eIn a recent blog post I mentioned that I have been thinking about novel applications to mathematics. One of the areas I have been exploring is the concept of \u003cstrong\u003enumerical impermanence\u003c/strong\u003e. And you\u0026rsquo;re probably thinking, \u0026ldquo;are you on drugs?\u0026rdquo; or \u0026ldquo;what the hell is that?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eSince moving to France in 2022 and having my world turned upside down with the move, and the deaths of my grandfather and the Queen, I have been leaning on impermanence to help me stay grounded and navigate the chaos of life. Mathematics, on the whole, is a grounded discipline, but it is also a discipline that is constantly evolving. The concept of impermanence is not new, but it is one that I have been thinking about in the context of mathematics and how we can apply it to more than just understanding life.\u003c/p\u003e","title":"Numerical Impermanence: An Introduction"},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""},{"content":"Hey blog!\nIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\nI have been reflecting recently on the systems we create and how they reflect our values. I have also been thinking about novel applications to mathematics, but that is for a different day.\nWhen it comes to the systems we build - be it software, hardware, or even social systems - they are a reflection of our philosophies and beliefs. The choices we make in the design and implementation of these systems can tell us a lot about what we value as individuals and as a society.\nWithin the context of data, there are many facets we can explore, from the data we choose to collect, the algorithms and transformations we apply, the way we present and visualise the data, the decisions we make about how to use the data, the way we communicate our findings, and the governance and culture we create around the data. Each of these facets can tell us something about our philosophies and beliefs.\nHere\u0026rsquo;s a principle I have been thinking about:\nThe structure and behaviour of our systems reflect the philosophies and beliefs of their designers and leaders.\nThis principle is a reliable diagnostic lens through which we can understand the systems we create. It is a reminder that our systems are not just technical constructs, but also social and cultural artefacts that embody our philosophies and beliefs.\nWhere the Principle shows up You seen it in tech:\nWhen social media platforms optimise for engagement at the cost of well-being (the irony of publicising this over social media), you\u0026rsquo;re seeing the philosophical trade-offs of growth-driven engineering cultures. You see it in data:\nWhen an organisation tracks productivity but ignores burnout or context, that\u0026rsquo;s a reflection of a belief that productivity is more important than the well-being of employees. You see it in leadership:\nThe transparency (or opacity) of decision-making, the tolerance of ambiguity, the comfort with dissent - these become baked into workflows, policies, even org charts. Every design decision is a value choice:\nWhat we track and what we ignore Who gets control and who gets surveilled What gets standardised and what gets left to intuition When a system emphasises competition over collaboration, or surveillance over trust, it\u0026rsquo;s not a coincidence - it\u0026rsquo;s a philosophical choice.\nWhat does this mean for us? This principle forces us to ask harder questions:\nWhat part of me is showing up in the system I\u0026rsquo;m designing or leading? What values am I encoding into culture, structure, or strategy - intentionally or unintentionally? What do our metrics say about what we believe matters? What does our resistance to change say about our beliefs and who we\u0026rsquo;ve become? My beliefs I\u0026rsquo;m going to state my philosophies and beliefs here, and I encourage you to do the same. This is a personal exercise, but I think it is important to share them with others. I want to be held accountable for my beliefs and philosophies, and I want to be able to hold others accountable for theirs.\nMy philosophies and beliefs have shaped me as a person and as a leader. They have influenced the way I think about data, technology, and leadership. They have shaped the way I approach problems and the way I interact with others. And they evolve over time as I learn and grow.\nI am a liberal and a Catholic. I believe that people are called to grow into the highest possible standards of excellence - not just in outcomes but in purpose.\nI believe that society, and the systems we create, has a duty: to keep people safe, private, and secure - not in fear but in freedom.\nEveryone is on a journey. Systems can\u0026rsquo;t define that journey for them. But they can support it - by offering guidance, clarity, and a space to grow.\nI believe we seek meaning. And I believe structure, thoughtfully designed, helps focus that search - not to constrain, but to shape. We do not stumble into meaning. We create it. We build it. We design it. And we do so in a way that is intentional and thoughtful.\nThis is why I believe systems should be shaped with:\nreflection - so we know what we are doing and why we are doing it understanding - so we can see ourselves and others in the process compassion - because no one is perfect, and no system will be either I don\u0026rsquo;t always get this right, especially as I have designed systems unconsciously. I have perhaps leaned into more Catholic elements of my upbringing than I would like to admit, such as hierarchy and rigid structure. I have also perhaps leaned into more liberal approaches which have neglected the need for structure and clarity. And in the future? These philosophies and beliefs will continue to evolve.\nDesign is never neutral The comforting lie of system design is that it is objective - that it\u0026rsquo;s just logic and reasoning, devoid of personal biases or societal influences. However, every design decision we make is inherently subjective, reflecting our values, beliefs, and the context in which we operate.\nEvery toggle, policy, and process says something about us. And when we leave that \u0026ldquo;something\u0026rdquo; unexamined, we risk perpetuating harm or building structures that calcify our worst qualities and instincts.\nA Call to Consciousness You don\u0026rsquo;t need to control every outcome or be responsible for every decision of a system to be able to apply this principle. You just need to acknowledge: your fingerprints are on the system.\nWhat kind of fingerprints do you want to leave?\nLead in a way you\u0026rsquo;d be proud to be mirrored. Build systems that tell a better story about who we are - and who we want to be. Be intentional about the systems you create and the philosophies and beliefs they reflect.\n","permalink":"http://localhost:1313/posts/strategy/what-our-systems-say-about-us/","summary":"\u003cp\u003eHey blog!\u003c/p\u003e\n\u003cp\u003eIt has been a while! Life happens and has a way of distracting us from some of the things we love. I have been busy with work and family - still posting content elsewhere, some of which I might share here in the future, others will be left to stay where they are. It\u0026rsquo;s valuable content in the context in which it was created, but not necessarily valuable in the context of this blog. I have been thinking about how to get back into the swing of things and I think I have a good topic for today.\u003c/p\u003e","title":"What Our Systems Say About Us"},{"content":"Self-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\nBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\nLet\u0026rsquo;s start with an analogy Sometimes it can be easier to convey ideas and concepts using analogies, so let\u0026rsquo;s use one. The most obvious analogy to use is that of restaurants and cooking. We\u0026rsquo;re all familiar with the concept of a restaurant, and we all know that a restaurant is a place where we can be served a limited choice of food. But there are different types of restaurants which cater for different styles of service. From traditional, sit-down restaurants through to cook-your-own food restaurants.\nSit-down Restaurants Sit-down restaurants are restaurants where the customer is seated at the restaurant and is served their food from a restricted menu. The preparation of the food is done by the professional kitchen staff, the food is served by professional waiting staff and all the customer has to do is choose what they want to eat.\nIf we translate that to analytics, we can say that all the end-user has to do is choose which report they want to look at and consume. Everything else is handled by the system and the data professionals in the background.\nConveyer Belt Restaurants Conveyer Belt Restaurants are prevalent in Japan and are making headway in the western world. Food is prepared by professional kitchen staff, but here the customer is served the food from a conveyer belt. There\u0026rsquo;s no waiting staff to serve them. The customer chooses what they want to eat based on what they see on the conveyer belt. The menu is still fairly restricted but the customer has flexibility in being able to choose what to eat when they\u0026rsquo;re ready.\nIf we translate that to analytics, this could be an end-user creating their own reports. All the hard work is done for them, they\u0026rsquo;re choosing visuals based on the data made available to them.\nBuffet Restaurants Buffet Restaurants have a wide range of options, usually across cultures and regions. The customer serves themselves from the buffet and the menu is fairly unrestricted. Food is still prepared by the professional kitchen staff, but the customer has a choice of what they want to eat and how to compose their own menu. For example, they could choose to have dessert on the same plate as their main meal. Most people would be horrified at the pairing of desserts and main meals on the same plate, but the buffet is a great way to get a taste of the world in a manner that suits individuals.\nTranslating that to analytics, and this is end-users creating their own semantic data models. The preparation of data is still done by other people, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports. This is the traditional remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics.\nCook-Your-Own Food Restaurants Cook-your-own food restaurants are fairly limited to Japan and Korea, but we\u0026rsquo;re seeing more and more of them in the west. As a result of the Covid-19 Pandemic, many sit-down restaurants pivoted to a cook-your-own offering in the shape of restaurant kits. In a cook-your-own food restaurant, the customer is given prepared ingredients and is then left to cook their own food. There are usually hot plates in front of the customer for the customer to cook the food on.\nBack in the analytics world, this would be allowing the end-user to curate their own data, and present it in their own way. We\u0026rsquo;re pushing the responsibility of curating data to the end-user and away from the data professionals. The professionals are still involved in preparing the data to the end-user, but the end-user is now responsible for curating the data and presenting it in a way that is meaningful to them.\nPrep and Cook at Home Ok, this is perhaps not a restaurant any more. There perhaps aren\u0026rsquo;t any customers. But here, the \u0026ldquo;customer\u0026rdquo; is preparing and cooking the food themselves. There are no kitchen staff, waiting staff. The \u0026ldquo;customer\u0026rdquo; is entirely responsible for the food and the way it is cooked. They might follow a recipe, they might not. This is the preserve of the generalist. The amateurs are in control. But this doesn\u0026rsquo;t mean that the food will be ugly and bland - \u0026ldquo;Not everyone can become a great artist; but a great artist can come from anywhere.\u0026rdquo;1\nIn analytics, this would mean the end-user is fully involved in the onboarding and ownership of analytical data. Data Professionals are not involved, but might offer support and guidance.\nThe Recipe Creator We\u0026rsquo;ve all scoured the internet looking for a recipe to follow, to come across an entire life story before we get to the recipe itself. Some of these recipes are great and push the boundaries of what we can do with food - contributing to the world. Others are not so great and are just a recipe. They are contributing to wider culture in establishing a new way of thinking about food.\nWith analytics, this would be end-users contributing to best practice and the wider data culture. We\u0026rsquo;re blurring the lines between traditional data professionals and end-users. The generalist is the professional, the specialist is the end-user.\nSelf-Serve Analytics We\u0026rsquo;ve explored the analogy above, and mapped the different self-serve levels to the different types of restaurants. Now let\u0026rsquo;s look at the different self-serve levels in more detail.\nLevel 0: Automated Reports This is traditional analytics. From an analytics maturity curve perspective, this is most immature level of analytics. End-users are spoonfed data and often told how to interpret the data. This assumes that there are fixed views of data that are defined and maintained by data professionals. If the end-user has any choice in how they interact with the data, it\u0026rsquo;s either through pre-defined slicers (if they exist) or they can choose a different report.\nAs it is the most rigid form of analytics, it also creates the most contention and friction between the end-user and the data professionals. The anecdotal stories we have of end-users getting frustrated having to wait weeks or months for a report to be generated, and the frustration of data professionals with the end-users for not having any understanding or appreciation of the complexities of the requirements or the skills required to produce the reports.\nAnd we have this frustration from both sides, resulting from the fact that the end-user has no ownership of the data.\nLevel 1: Authors Own Reports In an attempt to address the lack of ownership of the data, we\u0026rsquo;ve got a slightly more mature new level of analytics, in which end-users author their own reports from centralised data models. They have flexibility in choosing what data they want to visualise and how to interpret it. However, they are still dependent on the data professionals, who have ultimate ownership of the data model.\nHowever, there can still be frustration from the end-user and the data professionals. From the end-user, if they want new data or calculations in the model, they\u0026rsquo;re still dependent on the data professionals. From the data professionals, it still takes a lot of time and effort to update the data model. But, because we\u0026rsquo;ve shifted more ownership and responsibility from the data professionals to the end-user, both parties are likely to be less frustrated than they were before.\nLevel 2: Authors Own Semantic Data Model(s) Further addressing frustrations with lack of ownership of data, this maturity level is encapsulated by the remit of self-serve analytics tools, like Power BI, and is typically seen as the limit to self-serve analytics. End-users are responsible for creating their own semantic data models, calculations, visualisations and interpretations of data. The preparation and curation of data is still done by data professionals, but the end-user has the ultimate choice and decision about how that data model is constructed, what data goes into it and what it is visualised in the reports.\nEnd-users are still dependent on data professionals for the application of business logic to the data, so that it can be consumed and modelled in a way suitable for the end-user - so if they need to bring in new data to the model, they\u0026rsquo;re still dependent on the data professionals. Of course, there are methods available to end-users to bring in their own data, circumventing the need for data professionals, but this isn\u0026rsquo;t necessarily the best way to do it.\nWith this level of self-serve analytics, the end-user requires training and guidance from the data professionals to understand the tools and techniques they can use to create their own data models, as well as understanding what data is available to them, the provenance and quality of the data. With this transference of responsibility and ownership of data, also needs to come with a transference of skills and expertise, so that the end-user is empowered to create value from their data.\nIs there less frustration and friction between the end-user and the data professionals? Certainly a lot less than before, but where there is dependency there will always be friction.\nLevel 3: Curates Own Data Shifting the responsibility further on to the end-user, this level of self-serve analytics where there\u0026rsquo;s a lot more blending of responsibilities and roles between end-user and data professional. This area of data is traditionally the preserve of BI developers and the emerging role of Analytics Engineers - transforming cleansed source data into curated assets through the application of business logic. With this level, end-users are fully responsible for curating their own data and applying business logic to the data. They are not dependent on data professionals for the curation of the data, but they are still dependent for the onboarding and cleansing of data.\nThis feels like a natural limit for self-serve analytics, while we\u0026rsquo;re still in the realms of monolithic and centralised data architectures, such as data lakes, data warehouses, data lakehouses and so on. If we\u0026rsquo;re to shift the ownership of data further on to the end-user, we need to be able to do this without the need for data professionals and, in so doing, without the need for a centralised data platform.\nLevel 4: Onboards and Owns Data Now we\u0026rsquo;re in the realm of distributed, decentralised data architectures of the Data Mesh, and similar. The end-user is fully responsible for onboarding and ownership of their data. Data professionals no longer exist and have evolved into supporting a self-serve data platform. This data platform enables self-serve to encompass all facets of data - from onboarding and ingestion, through cleansing, modelling and serving.\nFor more information on the self-serve data platform, see this blog post on a deep dive into data mesh.\nLevel 5: Contributes to Best Practice and Wider Culture If we\u0026rsquo;ve got this far, we\u0026rsquo;ve got a solid understanding of the different self-serve levels. This last one might appear to be unattainable, but it\u0026rsquo;s the level of self-serve analytics that is the most important and doesn\u0026rsquo;t require any of the other levels to be achieved.\nIn this level, we still have end-users and data professionals, but some sections of end-users have evolved into extolling the value of the data, and contributing to how the organisation should use data in the form of best practice guidance and encouraging a positive data culture in the wider organisation.\nConclusion We\u0026rsquo;ve explored the different self-serve levels in more detail, and we\u0026rsquo;ve seen how they impact the way end-users and data professionals interact with each other. By shifting more responsibility and ownership towards the end-user, we\u0026rsquo;re reducing friction. But at the same time, the end-users need to be empowered to create value from their data through training and guidance from the data professionals.\nWe\u0026rsquo;ve also seen how varied the definitions for self-serve analytics are, and how we need to be clear on what we mean by self-serve analytics, as self-serve can be defined in different ways.\nFor organisations, it\u0026rsquo;s important to understand the different self-serve levels - recognise where they are and where they want to be - so that they can plot how to achieve their target self-serve level.\nAnton Ego, Ratatouille (2007)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/strategy/what-we-mean-self-serve/","summary":"\u003cp\u003eSelf-serve, or self-service, has been around in various guises for decades from Automatic Teller Machines (ATMs) through to self-serve analytics. With the advent of spreadsheeting tools, self-serve analytics has arguably been around for a long time, and with the more recent introduction of dedicated self-serve Business Intelligence tools, self-serve has become embedded in the analytics vernacular.\u003c/p\u003e\n\u003cp\u003eBut what do we really mean when we say \u0026ldquo;self-serve\u0026rdquo;? In my experience, we all have different interpretations and understanding of what we mean - and those interpretations will be specific to the context in which we operate. For some, self-serve could mean allowing the end-user to author their own reports. For others, it could mean allowing the end-user to be fully involved in the onboarding, curation and sharing of data.\u003c/p\u003e","title":"What do we mean by Self-Serve"},{"content":"Problem Interactive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\nFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\nSolution A simple solution to both problems, doesn\u0026rsquo;t yet exist. It could be possible to use SQL Serverless for the Warehouse clusters. However, at the time of writing, Serverless compute is still in private preview. And the equivalent for Engineering workloads still doesn\u0026rsquo;t really exist.\nTherefore, we need to have a method for warming up the clusters before they get used. And this is where the API comes in handy.\nThe Databricks API is incredibly powerful and allows you to programmatically control your Databricks experience.\nThe process for the programmatically scheduling the warm-up of Engineering and Warehouse clusters are the same but the API endpoints are different, therefore the code is different (notebooks below).\nEngineering Cluster Notebook SQL Warehouse Cluster Notebook Conclusion With our new notebooks, using the API, we can now set them on a schedule using a job cluster to start or stop at specific times of day.\nFor Engineering clusters with a streaming workload, this allows the clusters to be available when the streams start, as well as allowing the VMs some downtime for maintenance once streams stop.\nFor SQL Warehouse clusters, this allows the clusters to be available for users to refresh their reports and query data before they\u0026rsquo;ve started work - so no more waiting for clusters to start.\nThere are so many more uses for the Databricks API and we\u0026rsquo;ve just touched on a few here. If you\u0026rsquo;re using the API for a different use case, pop it in the comments below as I\u0026rsquo;m interested to know how other people use the API.\n","permalink":"http://localhost:1313/posts/databricks/cluster-scheduling/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eInteractive and SQL Warehouse (formerly known as SQL Endpoint) clusters take time to become active. This can range from around 5 mins through to almost 10 mins. For some workloads and users, this waiting time can be frustrating if not unacceptable.\u003c/p\u003e\n\u003cp\u003eFor this use case, we had streaming clusters that needed to be available for when streams started at 07:00 and to be turned off when streams stopped being sent at 21:00. Similarly, there was also need from business users for their SQL Warehouse clusters to be available for when business started trading so that their BI reports didn\u0026rsquo;t timeout waiting for the clusters to start.\u003c/p\u003e","title":"Scheduling Databricks Cluster Uptime"},{"content":"Context A project that I\u0026rsquo;m working on uses Azure Synapse Serverless as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a Delta file.\nOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of Stored Procedures in Synapse - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to VARCHAR(8000) in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\nWe have also assumed that everyone using a particular Synapse instance will need and want to be able to query the same datasets. Therefore, we\u0026rsquo;ve given controlled the access to the underlying data lake using the Managed Identity. This makes the deployments simpler, as there are fewer Azure AD objects that need the appropriate RBAC and ACL permissions - just the appropriate and relevant Managed Identity.\nLimitations There limitations for using automated deployments to Synapse Serverless are incredibly annoying, particularly if you are used to deploying other databases.\nDACPACs - the go-to method for deploying SQL Databases - don\u0026rsquo;t work. SQL Scripts do work, but not as one continuous script. Contributors to the Subscription aren\u0026rsquo;t automatically granted access to the resource, like with other resources. Solution Before we go into any solution, let\u0026rsquo;s take a moment to specify some pre-requisites as, without them this solution will not work.\nPre-requisites Service Principal used to deploy the ARM Template must have the User Access Administrator role assigned to it. You have to hand, or a way of finding out, the Azure AD Object IDs for the accounts you want to be a Synapse Workspace Admin. Let\u0026rsquo;s Build This Pre-requisites out of the way, let\u0026rsquo;s build this thing.\nARM Templates are the primary vehicle for the creation and configuration of the Synapse workspace. If you\u0026rsquo;re using something like Bicep or Terraform, the approach will be similar - although the syntax might be simpler.\nI won\u0026rsquo;t bore you with the specifics, there\u0026rsquo;s the code for that. Other than the creation of the Workspace, we\u0026rsquo;re adding the Managed Identity to the Data Lake as a Storage Blob Data Contributor. ACL definition can then be done at a container / folder level. We\u0026rsquo;re also assigning the Workspace Name, SQL User and Password down to Key Vault for use later on, as well as outputting the Workspace Name to DevOps pipeline.\nARM Template ARM Parameters Now that we have the name of the Workspace in the DevOps pipeline, we can use it in a PowerShell script to assign our accounts as Synapse Admins. If we don\u0026rsquo;t do this step, no one will be able to use the Workspace as the only accounts that automatically have access are the Managed Identity and the Service Principal that deployed the workspace.\nWorkspace set up and permissions granted, time to deploy the database! And I hope you like command line\u0026hellip;\nAs mentioned earlier, DACPACs don\u0026rsquo;t work and a SQL Script that contains all the objects, doesn\u0026rsquo;t work either. It\u0026rsquo;s like Synapse doesn\u0026rsquo;t recognise the ; command terminator. So we have to break the script into component scripts that do run together.\nCreate Database Create External Resources Create Schema Create Stored Procs And all this is orchestrated by an Azure DevOps Pipeline, written in yaml.\nDevOps Pipeline Yaml Conclusion Deploying a Database, and objects, to Synapse Serverless is frustrating, but it\u0026rsquo;s also possible. There are potentially alternatives, like the Synapse Workspace Deployment task in Azure DevOps, but I don\u0026rsquo;t have the appropriate permissions in this DevOps instance for me to install the module and try it out. Considering the poor reviews, I would assume that it might not do what I needed to do.\n","permalink":"http://localhost:1313/posts/sql/synapse-serverless-cicd/","summary":"\u003ch2 id=\"context\"\u003eContext\u003c/h2\u003e\n\u003cp\u003eA project that I\u0026rsquo;m working on uses \u003ca href=\"https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/overview-architecture\"\u003eAzure Synapse Serverless\u003c/a\u003e as a serving layer option for its data platform. The main processing and transformation of data is achieved using Databricks, with the resulting data being made available as a \u003ca href=\"https://delta.io/\"\u003eDelta file\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOur processes ensure that the Delta files are registered automatically within Databricks as Delta Tables, but there is no native way to register Delta objects in Synapse. Therefore, we\u0026rsquo;ve gone down a route of creating a series of \u003ca href=\"https://www.youtube.com/watch?v=B2jr6YRemxM\u0026amp;ab_channel=AdvancingAnalytics\"\u003eStored Procedures in Synapse\u003c/a\u003e - which can be called from Databricks - which register the Delta files as views within Synapse. There are performance considerations to be understood with this approach - namely the use of loosely typed data. For example, a string in Delta is converted to \u003ccode\u003eVARCHAR(8000)\u003c/code\u003e in Synapse. This approach isn\u0026rsquo;t for everyone, so use with caution.\u003c/p\u003e","title":"CI / CD With Synapse Serverless"},{"content":"Data is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\nImpact of Data Quality The impact of poor quality data has been estimated to have cost the U.S. economy $3 Trillion in 2016. We\u0026rsquo;re now 6 years on, the size of the U.S. economy is much larger; the volumes of data more vast; the complexity of that data has grown - so has the impact of bad data also grown in value? That is difficult to assess - because the original estimate was based on a load of assumptions like the time and number of employees engaged in correcting, or working around, poor data quality, as well as the assumed cost of poorly informed decisions - but for the sake of simplicity, we can assume that this cost to a single economy has also grown.\nWe can safely assume this because a lot of organisations do not have data quality at the top of the priority lists. Why might this be the case? Because monitoring data quality and correcting poor quality data is hard. If it was easy, every organisation would have a strategy and method for tracking and improving data quality.\nOften, inertia - driven by an overwhelming amount of information to the risks - sets in as it\u0026rsquo;s difficult to know where to start. So, where do you start?\nDetermine Data Quality Handily, there are industry defined dimensions for assessing data, as defined by the Data Management Association (DAMA).\nCompleteness Completeness is defined as a measure of the percentage of data that is missing within a dataset. For products or services, the completeness of data is crucial in helping potential customers compare, contrast, and choose between different product or services. For example, if a product description does not include an estimated delivery date (when all the other product descriptions do), then that \u0026ldquo;data\u0026rdquo; is incomplete.\nFor an organisation, incomplete data can have a profound impact on efforts like marketing campaigns. For example, if a customer doesn\u0026rsquo;t fill in their address there\u0026rsquo;s no way for them to be engaged with physical marketing.\nTimeliness Timeliness measures how up-to-date or antiquated the data is at any given moment. For example, if you have information on your customers from 2008, and it is now 2022, then there would be an issue with the timeliness as well as the completeness of the data.\nWhen determining data quality, the timeliness dimension can have a tremendous effect — either positive or negative — on its overall accuracy, viability, and reliability. The more up-to-date data is, the more relevant decisions and insight can become.\nValidity Validity refers to information that fails to follow specific company formats, rules, or processes. For example, many systems may ask for a customer\u0026rsquo;s birthdate. However, if the customer does not enter their birthdate using the proper format, think US and ISO date formats, the level of data quality becomes compromised.\nIntegrity Integrity of data refers to the level at which the information is reliable and trustworthy. For example, if your database has an email address assigned to a specific customer, and it turns out that the customer supplied a fake email address, then there\u0026rsquo;s an issue of integrity with the data.\nUniqueness Uniqueness is a data quality characteristic most often associated with customer profiles, such as a Single Customer View.\nGreater accuracy in compiling unique customer information, including each customer’s associated performance analytics related to individual company products and marketing campaigns, is often the cornerstone of long-term profitability and success.\nConsistency Consistency of data is most often associated with analytics. It ensures that the source of the information collection is capturing the correct data based on the unique objectives of the department or company, and doing so in a consistent manner. If the system is collecting pertinent data in one field and that field switches, that would not be consistent.\nMonitoring Data Quality Data Quality won\u0026rsquo;t be magically fixed over night and could take many months, or years, to iteratively improve. In order for the quality to improve, the quality of the data needs to be monitored.\nThere are plenty of off-the-shelf products, which can be bought for not insubstantial amounts, such as Informatica, Ataccama, and many others. There also remains an option to build a solution that suits your organisation. In a later post, we\u0026rsquo;ll explore one option for building a solution.\nTakeaway Regardless of which approach you take to monitor data quality, build or buy, I hope this post has given you a deeper understanding of what data quality is and why monitoring, and improving, data quality is important.\n","permalink":"http://localhost:1313/posts/strategy/data-quality-importance/","summary":"\u003cp\u003eData is among the most valuable assets for any organisation. Without data, the ability to make informed decisions is diminished. So it stands to reason that Data Quality is incredibly important to any organisation. If data doesn\u0026rsquo;t meet the expectations of accuracy, validity, completeness, and consistency that an organisation sets it, then the data could have severe implications for the organisation. Conversely, if data does meet those expectations, then it is a real asset that can be used to drive value across an organisation.\u003c/p\u003e","title":"Why Data Quality is Important"},{"content":"Problem Databricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\nBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\nSolution Schema inference in Auto Loader works by sampling the data. It will do this by either reading 50GB or 1,000 files it discovers - whichever threshold is crossed first.\nBy default, a schema will be inferred at every run but that\u0026rsquo;s not efficient, so we need to supply it with a schema location to store the inferred schema. This means that at every run, Databricks is going to retrieve that defined schema and use it.\nHowever, unless you set cloudFiles.inferColumnTypes to true in the config, it\u0026rsquo;ll try to infer the data types on every run, even though they have been defined in the schema. This can cause errors if it decides that a long is an int as it will produce a schema mismatch.\nThere\u0026rsquo;s a lot more to Schema inference on the Databricks website, including all the different options and how it relates to schema evolution.\nIn terms of taking that inferred schema and amending it to what you were expecting or using it to filter out sensitive data fields, there are two approaches to this:\nSchema Hints Amending the Inferred Schema File Schema Hints Schema hints are really useful if the schema Databricks infers for you has data types which you think or know are incorrect. When you provide a schema hint, this overrides the inferred schema for the fields you\u0026rsquo;ve provided hints for.\nIn this example, our inferred schema has detected our Date field as a String; Quantity as an Integer; and Amount as an Integer. But what if Date is a date and Amount is actually a float?\nThis is where the schema hint comes in handy. We don\u0026rsquo;t need to modify the entire schema, just need to give Databricks the necessary information to make the right choice.\n.option(\u0026#34;cloudFiles.schemaHints\u0026#34;, \u0026#34;Date DATE, Amount FLOAT\u0026#34;) Which will produce a schema of:\nThis is great for small amendments to a schema, particularly if it\u0026rsquo;s a large one. If you need to make significant changes then you might want to define the schema upfront or take a look at the other option\u0026hellip;\nAmending the Inferred Schema File On the first run of using schema inference, Databricks will output the schema to a _schema location of your choosing. This can then be referenced in the schema location option.\n.option(\u0026#34;cloudFiles.schemaLocation\u0026#34;, \u0026#34;/mnt/lake/schemaLocation\u0026#34;) CAUTION: The file that is output is not meant to be edited so proceed with care.\nThe file is often called 0, though could be called any form of integer if the schema has evolved, and contains two lines:\nThe version of the schema, represented by vN The schema in a json block If you have a large and / or complex schema, editing this json block could be a challenge. But if your large / and or complex schema contains sensitive fields, it may be necessary to edit this block.\nIn this example, we have sensitive fields (highlighted) that we do not want to have in our data platform because:\nDo not need them in our platform for analysis purposes Will cause a world of pain if they get compromised Therefore it\u0026rsquo;s safer to remove them from the platform before they get ingested. Removing them is a manual exercise of deleting each contained field definition block.\n{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;fieldName\\\u0026#34;,\\\u0026#34;type\\\u0026#34;:\\\u0026#34;dataType\\\u0026#34;,\\\u0026#34;nullable\\\u0026#34;:boolean,\\\u0026#34;metadata\\\u0026#34;:{}} Depending on the number of fields, this could significantly reduce the number of fields in your schema. Depending on what\u0026rsquo;s left over, it may be easier and safer to define the schema up front.\nWe\u0026rsquo;ve removed the fields from the schema, but the schema evolution functionality of Auto Loader can easily add those fields back in. Therefore, we need to explicitly tell Auto Loader to not perform schema evolution.\n.options(\u0026#34;cloudFiles.schemaEvolutionMode\u0026#34;: \u0026#34;none\u0026#34;) This means that any new columns, including the ones we have intentionally excluded, will not make their way any further into the data flow.\nConclusion We\u0026rsquo;ve seen two approaches to using (and abusing) the inferred schema functionality of Auto Loader. Schema inference significantly reduces the barriers to ingesting data but there are occasions when schema inference might need a little help. Hopefully, this post will have given you an idea of when and why you want to interfere with an automated process.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-inferred-schema/","summary":"\u003ch2 id=\"problem\"\u003eProblem\u003c/h2\u003e\n\u003cp\u003eDatabricks\u0026rsquo; Auto Loader has the ability to infer a schema from a sample of files. This means that you don\u0026rsquo;t have to provide a schema, which is really handy when you\u0026rsquo;re dealing with an unknown schema or a wide and complex schema, which you don\u0026rsquo;t always want to define up-front.\u003c/p\u003e\n\u003cp\u003eBut what happens if the schema that has been inferred isn\u0026rsquo;t the schema you were expecting or it contains fields which you definitely don\u0026rsquo;t want to ingest - like PCI or PII data fields?\u003c/p\u003e","title":"Using and Abusing Auto Loader's Inferred Schema"},{"content":"Problem Recently on a client project, we wanted to use the Auto Loader functionality in Databricks to easily consume from AWS S3 into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from Azure Storage Accounts and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\nWe followed the steps on the Microsoft Docs to load files in from AWS S3 using Auto Loader but we were getting an error message that couldn\u0026rsquo;t be easily resolved in the Azure instance of Databricks:\nshaded.databricks.org.apache.hadoop.fs.s3a.AWSClientIOException: Instantiate shaded.databricks.org.apache.hadoop.fs.s3a.auth.AssumedRoleCredentialProvider on : com.amazonaws.AmazonClientException: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/: No AWS Credentials provided by InstanceProfileCredentialsProvider : com.amazonaws.SdkClientException: The requested metadata is not found at http://169.254.169.254/latest/meta-data/iam/security-credentials/ Azure doesn\u0026rsquo;t have notions of an InstanceProfile but AWS does, so marrying the two cloud platforms was going to be a challenge.\nSolution We realised, through trial and error, that the role which had been provisioned for us in AWS would allow us to query data in S3 through Databricks using temporary credentials. The challenge for us would be to allow Databricks, and potentially other services, to use those temporary credentials in a secure and repeatable manner.\nTemporary Credential Generation Temporary Credentials have a lifespan, which can be minutes through to a couple of days, so we want to be able to refresh them regularly and reliably - either on a schedule or when necessary. But for other services to use the same credentials, we\u0026rsquo;ll want to store the temporary credentials in Azure Key Vault - so that they are secured.\nFor this, we looked towards Azure Functions because, if Auto Loader wouldn\u0026rsquo;t work with them then we could use Azure Data Factory as a fallback option to load data into Azure.\nFollowing the steps laid out in the Microsoft documentation, we created our Function in Python.\nPython Libraries For our function to work, we needed the boto3 library for AWS and a whole host of Azure libraries to connect to Key Vault securely.\nimport boto3 from azure.keyvault.secrets import SecretClient from azure.identity import DefaultAzureCredential from botocore.config import Config Functional Steps Connecting to Key Vault Our first step is to connect to Key Vault to retrieve the AWS Access Key ID and Secret Access Key of the role for which we are generating the temporary credentials for. Connecting Azure Functions to Key Vault, for secret retrieval is not transparent - luckily, there\u0026rsquo;s a great blog post by Daniel Krzyczkowski which sets out the steps for configuring your Function App.\nWe added the following Application Settings to the Function App, so that we could retrieve and update specific secrets in Key Vault:\nAZURE_CLIENT_ID AZURE_CLIENT_SECRET AZURE_TENANT_ID A circular reference to Key Vault might sound counter-intuitive, but it\u0026rsquo;s the only way I could see that would allow the app to update secrets.\nBringing back secrets from Key Vault required the following code:\ndef retrieveSecret(client, secretName): secret = client.get_secret(f\u0026#34;{secretName}\u0026#34;) return(secret.value) keyVaultName = os.environ[\u0026#34;KeyVaultName\u0026#34;] keyVaultCredential = DefaultAzureCredential() keyVaultClient = SecretClient(vault_url= f\u0026#34;https://{keyVaultName}.vault.azure.net/\u0026#34;, credential=keyVaultCredential) accessKeyId = retrieveSecret(keyVaultClient, \u0026#34;AWSAccessKey\u0026#34;) secretAccessKey = retrieveSecret(keyVaultClient, \u0026#34;AWSSecretKey\u0026#34;) Getting Temporary Credentials Now that we\u0026rsquo;ve got the main credentials for the AWS Role, we can create temporary credentials for the role using the assume role functionality.\nos.environ[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;] = accessKeyId os.environ[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;] = secretAccessKey botoConfig = Config( region_name = \u0026#39;eu-west-2\u0026#39;, signature_version = \u0026#39;v4\u0026#39;, retries = { \u0026#39;max_attempts\u0026#39;: 10, \u0026#39;mode\u0026#39;: \u0026#39;standard\u0026#39; } ) client = boto3.client(\u0026#39;sts\u0026#39;, config = botoConfig) response = client.assume_role( RoleArn=\u0026#39;arn:aws:iam::1234567890:role/role_name\u0026#39;, RoleSessionName=\u0026#39;AzureFunctionRefresh\u0026#39;, DurationSeconds=3600 ) This returned our temporary credentials as a nice JSON object:\n{ \u0026#39;Credentials\u0026#39;: { \u0026#39;AccessKeyId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SecretAccessKey\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;SessionToken\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Expiration\u0026#39;: datetime(2015, 1, 1) }, \u0026#39;AssumedRoleUser\u0026#39;: { \u0026#39;AssumedRoleId\u0026#39;: \u0026#39;string\u0026#39;, \u0026#39;Arn\u0026#39;: \u0026#39;string\u0026#39; }, \u0026#39;PackedPolicySize\u0026#39;: 123, \u0026#39;SourceIdentity\u0026#39;: \u0026#39;string\u0026#39; } We then parsed the credentials into variables, so that we could pass them through into Key Vault to update some already defined secrets.\ndef updateSecret(client, secretName, secretValue): updated_secret = client.set_secret(secretName, secretValue) credResponse = response[\u0026#39;Credentials\u0026#39;] tempAccessKeyId = credResponse[\u0026#39;AccessKeyId\u0026#39;] tempSecretAccessKey = credResponse[\u0026#39;SecretAccessKey\u0026#39;] tempSessionToken = credResponse[\u0026#39;SessionToken\u0026#39;] updateSecret(keyVaultClient, \u0026#34;TempAccessKeyId\u0026#34;, tempAccessKeyId) updateSecret(keyVaultClient, \u0026#34;TempSecretAccessKey\u0026#34;, tempSecretAccessKey) updateSecret(keyVaultClient, \u0026#34;TempSessionToken\u0026#34;, tempSessionToken) Getting Databricks to work Now that Key Vault had our all important temporary credentials, it was a matter of getting Databricks to work with them.\nOur first cell in Databricks was to initialise our temporary credentials and to set some environment variables, which would allow us to connect to S3.\nspark.conf.set(\u0026#34;fs.s3a.credentialsType\u0026#34;, \u0026#34;AssumeRole\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.stsAssumeRole.arn\u0026#34;, \u0026#34;arn:aws:iam::123456789:role/role_name\u0026#34;) spark.conf.set(\u0026#34;fs.s3a.acl.default\u0026#34;, \u0026#34;BucketOwnerFullControl\u0026#34;) AccessKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempAccessKeyId\u0026#34;) SecretKey = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSecretAccessKey\u0026#34;) SessionToken = dbutils.secrets.get(\u0026#34;ScopeName\u0026#34;, key = \u0026#34;TempSessionToken\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.aws.credentials.provider\u0026#34;, \u0026#34;org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\u0026#34;) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.access.key\u0026#34;, AccessKeyId) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.secret.key\u0026#34;, SecretKey) sc._jsc.hadoopConfiguration().set(\u0026#34;fs.s3a.session.token\u0026#34;, SessionToken) Note how we\u0026rsquo;re using the same Assume Role ARN as we used in the process to get the temporary credentials.\nWe can now, very easily use Auto Loader in the way it was intended. However, unlike other sources for Auto Loader, mounting an S3 Bucket with temporary credentials doesn\u0026rsquo;t work - so we have to specify the bucket and top-level directory we want to work with in the load.\ndf = (spark.readStream.format(\u0026#34;cloudFiles\u0026#34;) .option(\u0026#34;cloudFiles.format\u0026#34;, \u0026#34;json\u0026#34;) .schema(definedSchema) .load(\u0026#34;s3a://bucket/directory/\u0026#34;) ) And writing out is just as easy.\n(df.writeStream.format(\u0026#34;delta\u0026#34;) .option(\u0026#34;checkpointLocation\u0026#34;, \u0026#34;/mnt/lake/directory/_checkpoint\u0026#34;) .trigger(once=True) .start(\u0026#34;/mnt/lake/directory/\u0026#34;) ) Conclusion Configuring Databricks Auto Loader to load data in from AWS S3 is not a straightforward as it sounds - particularly if you are hindered by AWS Roles that only work with temporary credentials.\nThis is one way of getting it to work. If there wasn\u0026rsquo;t a need for other services to also connect to S3, I should think that the method to generate the temporary credentials could be rationalised significantly and to exist solely within Databricks.\nThanks for reading!\nUPDATE: It is much simpler to configure this directly in Databricks if you do not need to use the temporary credentials for other services in Azure.\n","permalink":"http://localhost:1313/posts/databricks/autoloader-s3-azure-databricks/","summary":"\u003ch1 id=\"problem\"\u003eProblem\u003c/h1\u003e\n\u003cp\u003eRecently on a client project, we wanted to use the \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader\"\u003eAuto Loader\u003c/a\u003e functionality in Databricks to easily consume from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-s3\"\u003eAWS S3\u003c/a\u003e into our Azure hosted data platform. The reason why we opted for Auto Loader over any other solution is because it natively exists within Databricks and allows us to quickly ingest data from \u003ca href=\"https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/auto-loader-gen2\"\u003eAzure Storage Accounts\u003c/a\u003e and AWS S3 Buckets, while using the benefits of Structured Streaming to checkpoint which files it last loaded. It also means we\u0026rsquo;re less dependent upon additional systems to provide that \u0026ldquo;what did we last load\u0026rdquo; context.\u003c/p\u003e","title":"Using Auto Loader on Azure Databricks with AWS S3"},{"content":"Background In a previous post, we explored what the data domains could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\nWe won\u0026rsquo;t go through every domain identified in the previous post, instead we\u0026rsquo;ll focus on the Customer Domain and the data consumers of Group Finance and Senior Management.\nAs a reminder, below is our hypothetical flow of data between the domains established in the previous post:\nCustomer Data Product A Data Product is the smallest component of a data mesh, composed of the code used to process and serve data; the data that is consumed as a product, along with its metadata; and the infrastructure upon which the code is deployed and the data, as a product, is stored.\nA domain can have many data products, just like it might have many operational data sources and systems.\nIn our Customer Domain, we have the Customer information in the source applications and the Customer information in the CRM application, as highlighted in the below image.\nBut the reality of data flows for the customer domain are likely to be more complex than that, just because there are customers coming from different source applications and there\u0026rsquo;s a business need to ensure that the data is mastered and we have a golden source of data for customers. This is to ensure that Customer A, who habitually shops in Country A can have the same experience when they shop in a different country and get assigned a different Customer ID.\nAt this point, our data product could be the Master Data Management (MDM) tool for Customer as it can be used by other solutions - not just those owned by the Customer Relationship Team. Like any good MDM tool and process, we want that data to flow back to originating source systems so that they have accurate and up-to-date data.\nOther teams and systems can also subscribe to the data in the MDM tool but there\u0026rsquo;s potential that the customer data and associated metrics can become distorted by teams and processes that don\u0026rsquo;t have the same intimate knowledge of the data as the Customer Relationship Team.\nOwnership of customer information resides with the team, so they should provide some analytical capabilities, such as serving a dimension for traditional consumers, or more interesting insights. Even if these are only used for the team\u0026rsquo;s own purpose, it is still providing value.\nSome of the analytical capabilities the team may offer, such as providing a view of segmentation by other domains, e.g. Product, will need to consume and transform data from other domains.\nThis is OK - mainly because the data isn\u0026rsquo;t being strictly duplicated as the context of the data is changing. The context is changing from this is a product in the product domain to product preference as a customer attribute.\nTherefore, in the customer domain we have the following potential data products:\nCustomer Mastered Data Customer Dimension Customer Segmentation Segmentation by {Domain} Group Finance Data Product Group Finance, in the traditional analytical model, are heavy users of analytical data. They need to know exactly how each unit within a business is performing, often making investment / divestment decisions based on the performance of individual units and the group as a whole.\nBut Group Finance are often consumers of data, rather than providers of data - other than as aggregated, formatted datasets to senior management.\nRemember that a data product has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable Discoverable Discoverable data means that a user can go into a tool, search for a term e.g. customer or profit margin and have everything related to that search term returned to them.\nSelf-Describing Once discovered, the data needs to tell the user what structure it is, the quality of the data within, ownership, lineage etc., and all of this is held within metadata.\nAddressable Now discovered and described, users need to be told where the data is and have the potential to access it consistent. Much like a request to an IP address.\nSecure Users in the organisation know that sensitive information can be found in certain datasets and they know where they can be found. However, not everyone should or need to have access to all data. Therefore it needs to be secured.\nTrustworthy Now that a user has access to data, they need to be assured of its accuracy and trustworthiness. Too often, too much time is spent by users reconciling and verifying data because they do not trust it. Therefore, the data products need to assure users that it can be trusted.\nInteroperable Interoperability of data means it should interact well with other data from other domains. This means that the data is useful outside its context and domain. If it\u0026rsquo;s just bound to the domain it can\u0026rsquo;t be used as a product to be consumed.\nIn most cases, as a consumer, all of those principles should be satisfied. But there might be times were a consumer needs to take that data and create something new.\nTherefore, the data product for Group Finance can go in one of two ways:\nData Virtualisation Domain Consolidation Data Virtualisation Data virtualisation allows users to retrieve data from the various domains without physically moving the data. The data still resides, physically, with the domains.\nThere is a risk here that the data provided by domains isn\u0026rsquo;t what is actually required by Group Finance and there is additional work to interpret the data. But in the data mesh, this is covered by a federated governance model which governs how data is provided and places the consumer (in this case Group Finance) at the core of service and requests.\nData Virtualisation would allow Group Finance to aggregate and format datasets to senior management in a way very similar to how they would in an analytical model; as well as providing them with the flexibility and agility of decoupling data pipelines from a central team and devolving that responsibility to domain teams.\nWith the domain using data virtualisation to consume data, the data product here are the queries to bring the data together, as well as the reports to visualise and share the data.\nDomain Consolidation Domain Consolidation is traditional analytics but, instead of taking the raw data from all the different domains, applying cleansing and transformations, and producing an enterprise view of the data - we\u0026rsquo;re taking the domain views of their data, in the form of served dimensions, and consolidating it into a consolidated view.\nThis consolidated view doesn\u0026rsquo;t necessarily mean a conformed view, as it can still retain the language of the domains for attributes. However, it could mean the change in context for the transactional elements of domains, such as sales. While the dimensions would be consolidated and could still reside in their domains (similar to Data Virtualisation), the facts would be transformed into a new context - with links to the dimensions. In effect, these facts would be conformed, as well as consolidated.\nIn this use case, there would either have to be new data created in the Group Finance domain to justify duplicating data and effort - such as supplementing the data with profit and loss, or there has been a breakdown in trust between domains and the federated governance model isn\u0026rsquo;t working.\nThe preference in most cases would be for pure consumers of data to employ data virtualisation because there would be fewer data management overheads than the alternatives. However, where there is a need to create new data - then there can be a strong case for physically consuming data from other domains.\nConclusion We\u0026rsquo;ve established what data products could look like for a single domain, as well as established how more traditional users of data would interact with the products - with a preference for data virtualisation.\n","permalink":"http://localhost:1313/posts/data-mesh/data-product-case-study-retail/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn a previous post, we explored what the \u003ca href=\"../data-domain-case-study-retail/\"\u003edata domains\u003c/a\u003e could look like for our fictional retailer - XclusiV. In this post, we will explore how the data products could work in this fictional case study, including how pure data consumers would handle the data - particularly those consumers who have a holistic view of an organisation (also a group of consumers for whom a traditional analytical model is perfect).\u003c/p\u003e","title":"Data Product Fictional Case Study: Retail"},{"content":"In previous posts we\u0026rsquo;ve understood what is Data Mesh and gone into greater detail with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\nFictitious Company: XclusiV XclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\nEach division has their own stores but, occasionally, there are flagship XclusiV stores that house products and staff from both divisions.\nThe retail operations are supported by separate manufacturing and buying processes, separate marketing teams; as well as centralised distribution, IT, Finance, and HR capabilities.\nXclusiV Analytical Challenges The analytical challenges that XclusiV faces can be summarised as follows:\nCentralised Data Warehouse that treats both divisions and all markets the same A central Data Engineering team that has limited capacity to support the development of new features requested by business teams Shadow IT has sprung up to try to overcome the capacity limitations of the Data Engineering Team but has created more confusion The business teams do not feel like they own the data Very few business teams want to own the data We\u0026rsquo;ve got two themes for our challenges: poor analytical performance (which will impact business performance) and lack of data ownership.\nQuality of data is quite good, so there isn\u0026rsquo;t the usual data governance concerns, other than ownership.\nHow could Data Mesh help? Due to the lack of ownership of data, a data mesh might not be able to resolve the challenges at XclusiV. However, embarking on a journey which, ultimately, empowers users will go a long way to establishing data ownership within domains.\nThe organisational structure doesn\u0026rsquo;t make this a simple use case for data mesh. In other examples, there\u0026rsquo;s a clear separation between domains: user domain; playlist domain; artist domain etc., but in this example, the separation between domains isn\u0026rsquo;t so clean. I think one of the reasons why there\u0026rsquo;s a clear domain separation in other examples is due to a microservices architecture. XclusiV haven\u0026rsquo;t got around to implementing a microservices architecture and are still using line-of-business applications.\nWe could separate domains into Sale, Product, Customer, Location, Stock etc., and that would work quite nicely if there was system alignment across markets (and that is a technological hurdle which shouldn\u0026rsquo;t be insurmountable). The following diagram details the data flow from one domain to another with the operational capabilities in blue and the analytical endpoints in green. You\u0026rsquo;ll notice that there\u0026rsquo;s a fair few dependencies, and this is a simplified model.\nRegardless of whether XclusiV has embraced microservices or retain LOB applications, the logical flow of data between arbitrary domains would be the same. But who should own what?\nDomain Ownership Domains can span multiple subject areas and have many dependencies. Let\u0026rsquo;s take Sales and Stock, as the driving forces for business - without stock, there\u0026rsquo;s no sales. If there are no sales, there\u0026rsquo;s no business.\nRetail Operations Team In this example, the operational capabilities and associated analytical endpoints for Sales and Stock domains come under the ownership of the Retail Operations Team. They\u0026rsquo;re tightly aligned to the creation of data and with the operational analytical demands, like Weekly Sales Report and Weekly Stock Position. The team is responsible for the input of data, the quality of the data, and associated output.\nBut surely the Retail Operations Teams is responsible for everything in the POS?\nYes, at the moment that is how XclusiV currently arranges its business. But the Retail Operations Team aren\u0026rsquo;t actually that interested in domains like Customer, or Location, or Product - so why would they want to own it? The team is responsible for the day-to-day operations of the stores and not concerned with product development, customer relationships - that\u0026rsquo;s the preserve of other teams.\nCustomer Relationship Team Unsurprisingly, the Customer Relationship Team should own the Customer domain, as well as the CRM domain.\nThe Customer Relationship Team wants to ensure that the right customers are targeted, that the right customer relationships are nurtured, and the quality of the customer data is accurate - specifically with regards to GDPR requests. A poor service, at any engagement point, will damage the reputation of XclusiV and reflect poorly on the Customer Relationship Team.\nFinance Finance are typically the gatekeepers for investment, as well as divestment. If sales are growing, there could be scope for opening up more stores or expanding product ranges. Conversely, if sales are not growing then stores could be closed and the product range could be more limited. Those decisions all rely upon Finance to make, with the advice from other departments.\nWhat about the Sales Team?\nThis is where it can get complicated. I recommend that it\u0026rsquo;s Finance who own these domains as they make the final decision but I do recognise that there is a lot of overlap.\nSales Team There is a lot of overlap between the Sales Team and the Retail Operations and the Finance Teams.\nThe Sales Team could be responsible for the Sales Planning Domain, but I argue that they are consumers of the domain as they are targeted by Finance on sales, rather than setting targets and driving decisions based on the data. Conversely, in another organisation it could be the Sales Team who owns the domain and Finance is the consumer.\nBuying Team The Buying Team are responsible for buying products for the various stores based on the trends and types of products that have performed well in the past and expected to perform well in the future.\nTherefore, the Buying Team owns the Inventory Planning Domain as it needs to plan for inventory it needs to buy. However, you\u0026rsquo;ll notice that the Buying Team does not have ownership of the Product Domain. But who does?\nMarketing Marketing owns the Product Domain. Why Marketing and not Buying or Manufacturing? Because Marketing are most keenly interested in the product as they are entirely responsible for the success of a product, how it is marketed and it\u0026rsquo;s performance (from a sales perspective). Owners of Product Domain but consumers of the Sales Planning Domain. Marketing sets the brief for products to be created.\nManufacturing is concerned about output and quality of the product but they\u0026rsquo;re not that interested in the success of a product. Buying is concerned about the required inventory of the product, but not the product itself. That\u0026rsquo;s why Marketing is, in my opinion, the correct owner of the Product Domain.\nCross-Divisions The examples above assume a single division - but we know that XclusiV is a multi-divisional organisation. How will it work with more than one division? We can have duplication of domains - to reflect the physical division of the organisation. This will also allow th organisation to scale if it decides to create or acquire new divisions or even divest divisions.\nXclusiV could re-organise and have shared domains across the divisions, but this would be counter to the principles of data mesh, as domains should be aligned to the organisation. In addition, to re-organise into shared domains across divisions wouldn\u0026rsquo;t be scalable and would impose some of the same challenges and restrictions of the current analytical structure.\nFor cross-divisional functions, like IT, we\u0026rsquo;ll explore their role in Self-Serve Analytical Platform Fictional Case Study: Retail and for cross divisional consumers, like Group Finance, we\u0026rsquo;ll explore how they would consume data in Data Product Fictional Case Study: Retail\n","permalink":"http://localhost:1313/posts/data-mesh/data-domain-case-study-retail/","summary":"\u003cp\u003eIn previous posts we\u0026rsquo;ve understood what is \u003ca href=\"../what-is-data-mesh/\"\u003eData Mesh\u003c/a\u003e and gone into \u003ca href=\"../data-mesh-deep-dive/\"\u003egreater detail\u003c/a\u003e with regards to the principles. In this next series of posts I want to use a fictional case study to explore how the underlying principles could work in practice. This post will introduce the fictitious company; the challenges it faces; and how the principle of decentralised data ownership and architecture, with domain alignment, would work.\u003c/p\u003e\n\u003ch2 id=\"fictitious-company-xclusiv\"\u003eFictitious Company: XclusiV\u003c/h2\u003e\n\u003cp\u003eXclusiV is a luxury retailer operating in multiple countries. It has two divisions, which operate almost as separate businesses, which we will call Division X and Division V. The Point-of-Sale (POS) and Enterprise Resource Planning (ERP) systems within each market that it operates is the same for each division, but the POS and ERP systems can vary between markets.\u003c/p\u003e","title":"Data Domain Fictional Case Study: Retail"},{"content":"Databricks recently released the public preview of a Data Generator for use within Databricks to generate synthetic data.\nThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\nDatabricks Labs Databricks Labs is a relatively new offering from Databricks which showcases what their teams have been creating in the field to help their customers. As a Consultant, this makes my life a lot easier as I don\u0026rsquo;t have to re-invent the wheel and I can use it to demonstrate value in partnering with Databricks. There\u0026rsquo;s plenty of use cases that I\u0026rsquo;ll be using, and extending, with my client but the one I want to focus on in this post is the Data Generator.\nData Generator The Data Generator aims to do just that: generate synthetic data for use in non-production environments while trying to represent realistic data. It uses the Spark engine to generate the data so is able to generate a huge amount of data in a short amount of time. It is a Python framework, so you have to be comfortable with Python and PySpark to generate data. But once data has been generated, you can then use any of the other languages supported by Databricks to consume it.\nIn order to generate data, you define a spec (which can imply a schema or use an existing schema) and seed each column using a series of approaches:\nBased on ID fields of base data frame Based on other columns in the spec Based on the value on one or more base fields Based on a hash of the base values Generated at random You can also iteratively generate data for a column by applying transformations on the base value of the column, such as:\nMapping the base value to one of a set of discrete values, including weighting Applying arithmetic transformation Applying string formatting The generator is currently installed to a cluster as a wheel and enabled in your notebook using:\nimport dbldatagen as dg From here we can create the specification for our synthetic data.\nspec = (dg.DataGenerator( spark # initialize a spark session ,name=\u0026#34;[name of dataset]\u0026#34; ,rows=[number of rows to be generated] ,partitions=[number of partitions to be generated] ,randomSeedMethod=None | \u0026#34;[fixed | hash_fieldname]\u0026#34; ,startingId=[starting value for seed] ,randomSeed=[seed for random number generator] ,verbose=True # generates verbose output ,debug=True # output debug level info ) .withColumn([column spec]) # inferring a schema .withSchema([Schema Definition]) # supplying a schema ) We can then build upon the spec, by applying transformations in an iterative way, reflecting the quality of the data (e.g. missing data) that you might expect in a production system, as well as the patterns of the data too.\nspec = (spec .withColumn(\u0026#34;email\u0026#34;, percentNulls=0.1, template=r\u0026#39;\\w.\\w@\\w.com|\\w@\\w.co.u\\k\u0026#39;) # insert a random word from the ipsum lorem word set .withColumn(\u0026#34;ip_addr\u0026#34;,template=r\u0026#39;\\n.\\n.\\n.\\n\u0026#39;) # insert a random number between 0 and 255 .withColumn(\u0026#34;serial_number\u0026#34;, minValue=1000000, maxValue=10000000, prefix=\u0026#34;dr\u0026#34;, random=True) # insert a random number using a range of values .withColumn(\u0026#34;phone\u0026#34;,template=r\u0026#39;+447dddddddddd\u0026#39;) # insert a random digit between 0 and 9 ) Finally, we use the build command to generate the data into a dataframe which can then be written and used by other processes or users.\ntestData = spec.build() testData.write.format(\u0026#34;delta\u0026#34;).mode(\u0026#34;overwrite\u0026#34;).save(\u0026#34;output path\u0026#34;) But the most incredible thing isn\u0026rsquo;t the ease in which synthetic data is generated. The most incredible thing is being able to generate synthetic data that relates to other synthetic data - consistent primary and foreign keys that you will most likely encounter in a production operational system.\nConclusion My previous experiences of generating synthetic data is a costly exercise, which largely isn\u0026rsquo;t worth the hassle. However, the Databricks Data Generator is easy to work with and makes generating synthetic data, through the multiple table approach, worthwhile.\nHowever, you still need access to production data in order to profile it and capture the quality of the data so that you\u0026rsquo;re making synthetic data reflective of the challenges that you\u0026rsquo;ll face once it comes to deploying your analytical solution to production and it starts ingesting and transforming production data in the wild.\nI cannot wait to start using this in every single project! Not only will it alleviate many concerns InfoSec might have with non-production environments, it will help improve knowledge of the data - due to the need to profile and model the data for generating a synthetic version of it.\n","permalink":"http://localhost:1313/posts/databricks/data-generator/","summary":"\u003cp\u003eDatabricks recently released the \u003ca href=\"https://github.com/databrickslabs/dbldatagen/releases/tag/v0.2.0-rc0-master\"\u003epublic preview\u003c/a\u003e of a Data Generator for use within Databricks to generate synthetic data.\u003c/p\u003e\n\u003cp\u003eThis is particularly exciting as the Information Security manager at a client recently requested synthetic data to be generated for use in all non-production environments as a feature of a platform I\u0026rsquo;ve been designing for them. The Product Owner decided at the time that it was too costly to implement any time soon, but this release from Databricks makes the requirement for synthetic data much easier and quicker to realise and deliver.\u003c/p\u003e","title":"Databricks Labs: Data Generator"},{"content":"In a previous post, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\nLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\nWhy do we need Data Mesh? In my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\nThis approach doesn\u0026rsquo;t scale very well, even though the underpining technology has made vast improvements to alleviate, or even mask, the impact of poor scalability. Even approaches that use metadata and configuration to automatically perform the bulk of the data process still end up falling foul of the same issue: scalability.\nHowever, I\u0026rsquo;m not referring to platform scalability, as most cloud platforms scale incredibly well both in terms of storage and compute scaling. The scalability issues I\u0026rsquo;m referring to are several points: a constantly changing data landscape (just think of all the different tools there are to work with data); a huge increase in data sources and volumes; a huge increase in demand for quality and usable data in organisations; huge increase in data use cases, coupled with diverse requirements for transformation and processing those use cases, and the need for organisations to respond quickly to change.\nIn a traditional analytical architecture (whether it\u0026rsquo;s a Data Warehouse, Data Lake, Data Hub or Lakehouse) the closer processes get to serving data from a central data repository, more dataset dependencies are introduced and the throughput becomes less.\nThis means that, while it is relatively easy to ingest data, it becomes harder to transform and serve data to the consumers who demand it, in a manner and time in which they desire. The business gets frustrated with IT, or the central data function, friction ensues and we end up with classic governance issues of shadow IT and dark data. All because the architecture we build no longer meets the needs of today, let alone the future.\nThe Data Mesh aims to overcome these challenges by embracing the following principles to perform at scale, while delivering quality, usable data to consumers:\nDecentralised data ownership and architecture Data Products enabled by Domain Driven Design Self-serve data platform infrastructure Federated Governance The big differentiators here are the decentralised operational model, supported by data as a product and the self-serve platform. For many data professionals, this is a huge change in the way of working because our architectural paradigm is shifting away from a centralised model to one which is decentralised and democratised.\nDecentralised Data Ownership and Architecture A foundational principle of data mesh is the decentralisation of ownership of data to those closest to it. Those closest to it are those in the business, often using the operational systems and also using analytical data. Responsibility and ownership of the data is devolved from a central function to business units and domains. Therefore any change to how a business domain organises itself is limited to the domain itself rather than impacting the entire organisation. This is referred to as the bounded domain context.\nTeams responsible for the business operation, e.g. CRM team responsible for Customers, are also responsible for the ownership and serving of the analytical data relating to their domain.\nBecause our organisations are often separated based on domains, our architecture needs to be arranged to support that. The interfaces of a domain needs to enable operational capabilities, e.g. registering a customer, but also provide an analytical endpoint, e.g. Newly registered Customers over last X months, to consumers. This means that domains must be able to provide these capabilities independently of other domains - essentially decoupling the analytical process at a domain level, much in the same way that microservices have decoupled operational capabilities at the domain level.\nIn the diagram above, we have the context of the domain - being the team and the systems they own - an operational capability (represented by O), and an analytical endpoint (represented by A).\nWhilst we\u0026rsquo;ve decoupled the analytical process at the domain level, we haven\u0026rsquo;t removed dependencies as there will exist dependencies between domains of operational and analytical endpoints, particularly of business critical domains.\nThere is, however, an assumption that operational systems have been built and maintained in-house for there to be this organisational alignment by domain, as well as a seamless sharing of data in domains. And, for some domains such as Customer and Product, it might also require significant investment in mastering data - so that there is a consistent view of these critical domains across systems.\nData Products A Data Product is a fundamental part of the data mesh and the smallest component of it. It contains the code used to ingest, transform and serve that data; the data that is to be consumed and its metadata, describing its schema, quality metrics, access control policies, etc; the infrastructure upon which the code is deployed, and the data to be stored, processed and accessed.\nA data product is bounded by the context of a domain and a domain can have many data products, just like it might have many operational systems. Very simply put, the following diagram is of a data product in the context of a domain.\nBut that\u0026rsquo;s not all there is to a data product. It is much more than a composition of code, data and infrastructure. It has the following core principles associated with it:\nDiscoverable Self-describing Addressable Secure Trustworthy Interoperable The reason why a data product has those core principles is because it makes it easier to use. What\u0026rsquo;s the point of building something if it\u0026rsquo;s not going to be used? Also, a data product can be used by other domains - so we need to treat any other domain just like any other customer. In fact, there\u0026rsquo;s no fundamental difference between a domain customer and any other data customer.\nSelf-Serve Data Platform A Self-Serve Data Platform is effectively Platform-as-a-Service with Infrastructure as Code principles, with some additional abstraction to lower the barriers of entry needed to build data products.\nLike with any architecture, there\u0026rsquo;s a large amount of infrastructure that\u0026rsquo;s needed to be deployed so that a data product can be built, deployed, executed, monitored and accessed. Infrastructure management is specialised skill-set that could be too prohibitive to have in every domain, particularly if there are hundreds of domains in an organisation. Therefore, the platform becomes a shared asset to enable domain autonomy.\nThere are some capabilities that the data platform should provide to domains:\nScalable, Distributed Storage Scalable, Distributed Query Engine Data Pipeline Implementation and Orchestration Identity and Control Management Data Product Code Deployment Data Product Discovery, Catalogue Registration, Lineage, etc. Data Product Monitoring / Alerting / Logs A typical workload on a shared self-service data platform infrastructure could look like the following diagram: incoming data (batch or streaming) gets ingested and processed and stored into the data structure that defines the data product, whether that is columnar or object. At the other end of the workload, an incoming request for data hits a web service, which then orchestrates a series of processes against the data product storage area, to then return that data to the customer. 1\nThis is highly abstracted to allow any technology to play the role of an ingestion service, processing engine, storage provider, web service, etc. While the platform needs to domain agnostic, it should also aim to be vendor agnostic and embrace Open Standards, Open Protocols and Open-Source Integrations. This allows the domain teams to be truly autonomous in choosing the tools to best meet their needs and skills - although, a word of caution here, as the selection of tooling should also be done in consultation with the data platform team, as they will be supporting the underlying infrastructure - including how the tooling interacts with the rest of the infrastructure.\nFederated Governance A data mesh is, by nature, a distributed, decentralised architecture - with autonomy built into the architecture - largely applying the principles of software architecture to data architecture. However, because there are dependencies between domains and their data products, there is a needs for these autonomous and independent products to interoperate at scale. This, therefore, requires a robust governance model that embraces self-serve and autonomy.\nThis is requires a Federated Governance model, that has contributions from data product owners and data platform product owners - creating and adhering to a common set of rules to be applied to all domains and their interfaces - to ensure interoperability. In a traditional governance model, this is similar to a Governance Council but the difference is that it is expanded to incorporate product owners.\nIn any federated system there will be challenges to maintain balance between what needs to be agreed globally and what the individual products have autonomy over. Ultimately, global concerns should be centered around interoperability and discovery of data products. Anything that will have an impact across domains will, most likely, become a focus of global concern and agreement, while elements that can be bounded by the domain context, such as the data model, would remain in the preserve of the domain. However, there might be standards set out by global that would need to be applied at the domain - to ensure interoperability and discovery.\nConclusion The data mesh is supported by four principles:\nDecentralised data ownership and architecture - so that the ecosystem can scale as the volume of data, number of use cases, and access models change over time. Data Products enabled by Domain Driven Design - so that consumers can easily discover, understand and consume high quality data. Self-serve data platform infrastructure - so that domain teams can create data products without the complexity of needing to support or understand the underlying platform. Federated Governance - so that consumers can get value from independent data products. Ultimately, the data mesh brings analytical and operational data closer together, as well as enabling capabilities and architecture patterns from software engineering in data engineering - with principal focus on interoperability.\nThe diagram is inspired from Lena Hall\u0026rsquo;s session at the Data \u0026amp; AI Summit, May 2021. See the session recording on YouTube.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/posts/data-mesh/data-mesh-deep-dive/","summary":"\u003cp\u003eIn a \u003ca href=\"../what-is-data-mesh/\"\u003eprevious post\u003c/a\u003e, we laid down the foundational principles of a Data Mesh, and touched on some of the problems we have with the current analytical architectures. In this post, I will go deeper into the underlying principles of Data Mesh, particularly why we need an architecture paradigm like Data Mesh.\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s start with why we need a paradigm like Data Mesh.\u003c/p\u003e\n\u003ch2 id=\"why-do-we-need-data-mesh\"\u003eWhy do we need Data Mesh?\u003c/h2\u003e\n\u003cp\u003eIn my previous post, I made the bold claim that analytical architectures hadn\u0026rsquo;t fundamentally progressed since the inception of the Data Warehouse in the 1990s. That\u0026rsquo;s three decades of the same thinking: ingest data into a central data repository, clean and transform data, serve data for consumption.\u003c/p\u003e","title":"Data Mesh Deep Dive"},{"content":"To be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\nAnalytical Generations The first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\nThe second generation of analytics is the Data Lake. While relatively in its infancy, having being incepted in 2010, the Data Lake was meant to try to solve some of the problems that Data Warehousing faced - namely analytics at scale in a big data era. Analytics became much more than the traditional Business Intelligence (BI) problem of historical trends but now came to encompass data science, of which the Data Lake prides itself on enabling. However, like the Data Warehouse, the Data Lake is maintained and supported by a small number of people in an organisation with a very niche, and in-demand, skill-set.\nThe third, and current, generation of analytics is the Data Lakehouse paradigm. While building on the advantages of both the Data Warehouse and the Data Lake, the Data Lakehouse suffers from the deficiencies of the previous generations: centralised, monolithic and needs a highly specialised skillset to develop and support it.\nThere\u0026rsquo;s going to be a lot of people who will hate me for this, but there has been no fundamental progress since the introduction of the Data Warehouse. The patterns are still the same, and the challenges that were seen then are still seen now. This is because, fundamentally, the logical architecture is the same: ingest data into a central location; apply conformed cleansing, enrichment and transformation to source data so that it is, hopefully, trustworthy; and serve those conformed datasets to a wide audience of consumers with a diverse set of needs. And this architecture pattern doesn\u0026rsquo;t scale very well - the larger the volumes, the more complex the data, etc., - the longer it takes to serve data to consumers.\nEven the configuration-based and metadata driven platforms will still struggle to serve consumers in a timely manner because there is still an end-to-end dependency between ingesting data, processing data, and serving it to consumers. That pipeline approach, like any pipe, introduces capacity constraints which limit the ability to scale and meet the demands of consumers and new data sources.\nThe crux of the problem is having a monolithic architecture which, due to its nature, produces monolithic support structures - a central super-specialised team of engineers - and monolithic architectures are what we want to overcome. Tearing down monolithic structures is the reason why we use cloud platforms; why we use multi-repos in our development; why we\u0026rsquo;re embracing (reluctantly or otherwise) self-serve analytics. But these only chip away at the edge because we\u0026rsquo;re not challenging the architectures we\u0026rsquo;re building.\nCrude Introduction What is a Data Mesh? Data Mesh is new and was introduced in a few articles by Zhamak Dehghani, starting in May 2019. The first article, How To Move Beyond a Monolithic Data Lake to a Distributed and Data Mesh, and the second article, Data Mesh Principles and Logical Architecture, form the foundational thought pieces on data meshes. Very simply, and crudely, a Data Mesh aims to overcome the deficiencies of previous generations of analytical architectures by decentralising the ownership and production of analytical data to the teams who own the data domain. It is a convergence of Distributed Domain Driven Architecture, Self-serve Platform Design, and Product Thinking with Data.\nEssentially applying the learnings from operational systems, of applying domain driven design, so that ownership of data is domain oriented. For example, a CRM team will own the Customer domain and all the data within it. Instead of data from all domains flowing into a central data repository, the individual domains curate and serve their datasets in an easily consumable way. A much closer alignment between operational and analytical data.\nProduct Thinking So, we\u0026rsquo;ve taken away data ownership and data pipeline implementation away from a centralised team of high-specialised people and put it into the hands of the business domains. How are we going to deal with accessibility, usability and harmonisation of the newly distributed datasets? This is where product thinking comes in, and providing data as Application Programming Interfaces (APIs) to the rest of the organisation, so that higher order value and functionality can be built on top of it. Alongside these APIs, there needs to be provided discoverable and understandable documentation; sandboxes; and tracked quality and adoption KPIs. Assets become products. Other teams become customers.\nSharing Between Domains What about the need to share across domains? Instead of the pull and ingest method, favoured by traditional data platforms, domains enable a serve and pull method, allowing other domains to pull and access datasets they need access to. And that might duplicate data, but in the era of cheap storage why should that be a concern - as long as the data from the originating domain can be trusted, it shouldn\u0026rsquo;t be a concern. Particularly as the served data that is consumed shouldn\u0026rsquo;t need to go through any additional cleansing or transformation.\nEach domain is responsible for the cleansing, de-duplication and enrichment of their data so that they can be consumed by other domains, without the replication of cleansing. Clean once and use as many times as you want in an organisation. This means that there now becomes a need for Service Level Agreements (SLAs) for the data a domain provides, including timeliness, error rates and many other Key Performance Indicators (KPIs).\nCentral Governance How do data products become discoverable? A common approach is to have a data catalogue of every data asset product (formerly known as asset) along with the relevant and appropriate metadata such as data owners, lineage, data profiling results, etc. A Data Catalogue makes discovering data easy and it is one of the few centralised capabilities that is not only permitted but encouraged.\nOnce discovered, the data product needs to have a unique address, following global standards and conventions (as laid out by a central governance team), that can easily be accessed by other data domains.\nHowever, with any system involving data, there still exists the concern with master data management. How is an Customer identified? What is a Customer? What attributes define a Customer? And this can only really be solved by a central governance function because, while the Customer domain might own the customer data, customer will be consumed and used by other domains - so there is a need for common agreement across some crucial data domains and datasets, often with a large number of dependents.\nCentral governance coupled with central security, like Identity Access Management (IAM) and Role Based Access Control (RBAC), allows data to be secured and for data products to be accessed securely. Policies for accessing data products need to be defined centrally and implemented locally, in the domain.\nSelf-Serve Platform Shifting the responsibility of data ownership and producing data products from a central team to distributed domain oriented teams could appear to be a huge burden, especially if they are to be responsible for the platform that hosts their data products. Fortunately, a Data Mesh can use a centrally provisioned infrastructure platform - whereby the pipeline engines, data storage and any other commonly used infrastructure can be used by all teams to develop their data products without having to worry about infrastructure as well. This responsibility for the shared platform belongs to a data infrastructure team all so that the domain teams have the necessary technology to \u0026ldquo;capture, process, store and serve their data products\u0026rdquo;.\nIt does mean that the underlying infrastructure needs to be domain agnostic, much like traditional data platforms, but also to provide an abstraction layer that hides all the underlying complexity of the infrastructure so that it can be used in a self-serve manner. And a lot of this can be achieved through some of the existing practices, such as configurable and metadata driven ingestions, auto-registration of data products in the data catalogue, and other automation capabilities favoured by engineers.\nParadigm Shift The data mesh is intentionally designed as a distributed architecture, enabled by a self-serve data infrastructure platform and a central governance function applying standards for interoperability. It is an ecosystem of data products that mesh together.\nIn an ideal world, there will be no more centralised engineering teams. Long live the citizen data engineer. No more highly specialised skillsets. Long live the generalist. No more centralised data platforms. Long live distributed data products. No more ingestion. Long live serving. No more ETL. Long live data discovery. Long live central governance.\nClosing Thoughts The data mesh is an interesting paradigm and, because it is so new, poses more questions than there are answers currently available, such as:\nHow does data mesh work in organisations which still have Line of Business aligned operational systems, as opposed to domain oriented operational systems? How much assumption does this paradigm place on in-house built operational systems versus off-the-shelf operational systems? How much organisational change is required to reorient an organisation to be domain driven? What level of data governance maturity is required before embarking on the data mesh? I like the potential it offers, such as decentralising data ownership and enabling more people to be involved in the curation of data. I LOVE the emphasis on data governance and how it plays a central role to enabling data mesh.\nThe shift in thinking means data engineering, and everything analytics, needs to start aligning much more closely with software engineering so that architectures like data mesh can be realised, as well as making more traditional architectures much more performant. While I can\u0026rsquo;t see widespread adoption of the data mesh any time soon, I would like to see some of the principles behind the data mesh adopted so that it becomes easier to adapt and change to new ways of thinking and new ways of working.\n","permalink":"http://localhost:1313/posts/data-mesh/what-is-data-mesh/","summary":"\u003cp\u003eTo be able to properly describe what Data Mesh is, we need to contextualise in which analytical generation we currently are, mostly so that we can describe what it is not.\u003c/p\u003e\n\u003ch2 id=\"analytical-generations\"\u003eAnalytical Generations\u003c/h2\u003e\n\u003cp\u003eThe first generation of analytics is the humble Data Warehouse and has existed since the 1990s and, while being mature and well known, is not always implemented correctly and, even the purest of implementation, comes under the strain of creaking and complex ETLs as it has struggled to scale with the increased volume of data and demand from consumers. This has led to systems being maintained by a select few in an organisation who are able to understand the system, in its entirety or even partially. Where once it provided value, the benefits posed to many mature organisations is, perhaps, diminishing.\u003c/p\u003e","title":"What is Data Mesh?"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-modelling/tabular-automation/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Tabular Automation with TMSL and PowerShell. By now, I was a seasoned speaker but was still quite nervous, especially as the attendees of my session included members from Microsoft\u0026rsquo;s development team. Feedback from them, was that it was nice to demonstrate how TMSL could be used outside what it was originally intended to do.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbitsvideo.streaming.mediaservices.windows.net/df3d0ded-fad6-4c2e-a0d1-544c3eb5ab0d/5895_1920x1080_AACAudio_5860.mp4\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Tabular Automation with TMSL and PowerShell: SQL Bits"},{"content":"At SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\nVideo to the session can be found on the SQL Bits website.\n","permalink":"http://localhost:1313/posts/data-lake/deep-dive-sql-bits/","summary":"\u003cp\u003eAt SQL Bits, Europe\u0026rsquo;s largest data conference, I presented a Deep Dive Into Data Lakes. It was my first solo speaking session and was the 08:00 session on the Saturday - right after the Friday night party.\u003c/p\u003e\n\u003cp\u003eVideo to the session can be found on the \u003ca href=\"https://sqlbits.com/Sessions/Event16/A_Deep_Dive_Into_Data_Lakes?\"\u003eSQL Bits\u003c/a\u003e website.\u003c/p\u003e","title":"Deep Dive Into Data Lakes: SQL Bits"},{"content":"Data Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\nWhat is a Data Lake? Before we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\nData Acquisition Data Management Data Delivery / Access A Data Lake is similar to a Data Warehouse in these regards. It is an architecture. The technology which underpins a Data Lake enables the architecture of the lake to flow and develop. Conceptually, the architecture of a Data Lake wants to acquire data, it needs careful, yet agile management, and the results of any exploration of the data should be made accessible. The two architectures can be used together, but conceptually the similarities end here.\nConceptually, Data Lakes and Data Warehouses are broadly similar yet the approaches are vastly different. So let’s leave Data Warehousing here and dive deeper into Data Lakes.\nFundamentally, a Data Lake is just not a repository. It is a series of containers which capture, manage and explore any form of raw data at scale, enabled by low cost technologies, from which multiple downstream applications can access valuable insight which was previously inaccessible.\nHow Do Data Lakes Work? Conceptually, a Data Lake is similar to a real lake – water flows in, fills up the reservoir and flows out again. The incoming flow represents multiple raw data formats, ranging from emails, sensor data, spreadsheets, relational data, social media content, etc. The reservoir represents the store of the raw data, where analytics can be run on all or some of the data. The outflow is the analysed data, which is made accessible to users.\nTo break it down, most Data Lake architectures come as two parts. Firstly, there is a large distributed storage engine with very few rules/limitations. This provides a repository for data of any size and shape. It can hold a mixture of relational data structures, semi-structured flat files and completely unstructured data dumps. The fundamental point is that it can store any type of data you may need to analyse. The data is spread across a distributed array of cheap storage that can be accessed independently.\nThere is then a scalable compute layer, designed to take a traditional SQL-style query and break it into small parts that can then be run massively in parallel because of the distributed nature of the disks.\nIn essence – we are overcoming the limitations of traditional querying by:\nSeparating compute so it can scale independently Parallelizing storage to reduce impact of I/O bottlenecks There are various technologies and design patterns which form the basis of Data Lakes. In terms of technologies these include:\nAzure Data Lake Cassandra Hadoop S3 Teradata With regards to design patterns, these will be explored in due course. However, before we get there, there are some challenges which you must be made aware of. These challenges are:\nData dumping – It’s very easy to treat a data lake as a dumping ground for anything and everything. This will essentially create a data swamp, which no one will want to go into.\nData drowning – the volume of the data could be massive and the velocity very fast. There is a real risk of drowning by not fully knowing what data you have in your lake.\nThese challenges require good design and governance, which will be covered off in the near future.\nHopefully this has given you a brief, yet comprehensive high-level overview of what data lakes are. We will be focusing on Azure Data Lake, which is a management implementation of the Hadoop architectures. Further reading on Azure Data Lake can be found below.\nFurther Reading Getting Started With Azure Data Lake Store\nGetting Started With Azure Data Lake Analytics and U-SQL\nData Lake Overview\nThis post was originally published at Adatis on the 13th of May 2016.\n","permalink":"http://localhost:1313/posts/data-lake/introduction/","summary":"\u003cp\u003eData Lakes are the new hot topic in the big data and BI communities. Data Lakes have been around for a few years now, but have only gained popular notice within the last year. In this blog I will take you through the concept of a Data Lake, so that you can begin your own voyage on the lakes.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-data-lake\"\u003eWhat is a Data Lake?\u003c/h2\u003e\n\u003cp\u003eBefore we can answer this question, it’s worth reflecting on a concept which most of us know and love – Data Warehouses. A Data Warehouse is a form of data architecture. The core principal of a Data Warehouse isn’t the database, it’s the data architecture which the database and tools implement. Conceptually, the condensed and isolated features of a Data Warehouse are around:\u003c/p\u003e","title":"Introduction to Data Lakes"},{"content":"What comes to your mind when you hear the words forecasting, forecasts etc?\nInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\nForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\nWhy use forecasting? Nearly every function within a company needs an estimate of what the future will look like as a foundation to create and update plans. For example:\nMarketers need forecasts to estimate the expected demand that will be generated from their activities.\nSalespeople use forecasts to understand what volumes are expected in each period and to evaluate the sales pipeline activity to make sure that it supports those expectations.\nManagers in the supply chain use forecasts to make timely purchasing requests, develop production plans, evaluate capacity needs, and develop logistical plans.\nFinance professionals use forecasts to prepare and understand financial plans. They also use them to report on earnings expectations.\nWhat can be forecast? The predictability of an event or a quantity depends the following conditions:\nHow well we understand the factors that contribute to it; For example, in energy consumption the change in temperature will have an impact on the amount of energy we use to heat our homes. How much data is available; Generally, the more data you have to hand the more accurate your forecasts will be. Whether the forecasts can affect the thing we are trying to forecast. This is the principle of self-fulfilling forecasts. For example, if you publicly forecast a share price buyers will adjust their behaviour in order to achieve the forecasted price. Often in forecasting, a key step is knowing when something can be forecast accurately, and when forecasts will be no better than tossing a coin. Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again.\nForecasting methods Average Approach The prediction of all future values are the mean of the past values. Naive Approach The prediction of all future values is that of the last observed value Approach is cost effective and provides a benchmark for more sophisticated models Drift Method A variation of the Naive method, whereby the increase or decrease (drift) is the average change seen across the historical data Seasonal Naive Accounts for seasonal changes in the data and sets each prediction to the last observed value in the same season Autoregressive Integrated Moving Average (ARIMA) Used to apply a better understanding of the data or to predict future points Exponential Smoothing\nApplies smoothing to the data in order to remove noise Three types Simple Exponential Models level Double Exponential Models level and trend Triple Exponential Models trend and seasonal components Probalistic\nAssign a probability value to each of a variety of different outcomes, and the complete set of probabilities represents a probability forecast. Most common example is in weather forecasting, but can be used in energy consumption, sports betting and population. Qualitative Approaches Typically asking people, usually experts, what they think is going to happen Can be interpreted as a crowd sourced forecast Forecast Planning Forecasting is a common statistical task which helps to inform decisions about scheduling of staff, production, etc., and provides a guide for strategic planning. Forecasting is often used in conjunction with planning and setting goals.\nForecasting is about the process of making predictions of the future based on past and present data and analysis of trends.\nGoals are what you would like to happen. You should plan how you are going to achieve them and forecast to see if they are realistic.\nPlanning is a response to forecasts and goals. It involves determining an appropriate response to make forecasts match goals.\nThe application of these three activities sets the length of time you want to forecast.\nShort-term forecasts: The sort of activity this is needed for is scheduling of staff, production, transportation etc. Demand forecasts are often required\nMedium-term forecasts: Needed to plan future resource requirements, such as hiring of staff and ordering materials and parts.\nLong-term forecasts: Used in strategic planning.\nConclusion Forecasting is a really powerful tool to add to your toolkit in order to better understand the future and to better accommodate what that forecast might bring so that you can grow your business in the best way possible.\nThis post was originally published at Adatis on the 16th of November 2015.\n","permalink":"http://localhost:1313/posts/data-science/forecasting-methods-principles/","summary":"\u003cp\u003eWhat comes to your mind when you hear the words forecasting, forecasts etc?\u003c/p\u003e\n\u003cp\u003eInvariably, you’ll think of weather forecasts. But forecasts are much more than that.\u003c/p\u003e\n\u003cp\u003eForecasting is the process of making predictions of the future based on past and present data and analysis of trends. It’s a process that has existed for millennia, though often with dubious methodologies… Instead of looking in to a crystal ball to predict the future we are going to employ the power of statistics!\u003c/p\u003e","title":"Forecasting: Methods and Principles"},{"content":"Some systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\nThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\nThe basis for rebasing exchange rates uses the technique of triangulation arbitrage. The technique is most often used within the banking system and more information can be found here: http://en.wikipedia.org/wiki/Triangular_arbitrage\nIn principle you deal with two known exchange rates and one unknown.\nFor example, if you have an exchange rate base of USD and know that the GBP/USD exchange rate is 1.54631 and the EUR/USD exchange rate is 1.11470 and wish to rebase from USD to EUR. You would begin by finding the inverse rates of GBP/USD (1/1.54631 = 0.6467), multiply by the EUR/USD rate (0.6467*1.11470 = 0.72087649) which produces the EUR/GBP and then find the inverse (1/0.72087649 = 1.3872) to produce the GBP/EUR rate. In order to find the USD/EUR exchange rate one simply finds the reverse of the EUR/USD rate (1/1.11470 = 0.8971).\nThat might sound simple, but most exchange rates are held in a table across a range of dates. This complicates the calculation somewhat. I\u0026rsquo;ve used CTEs because I find that it makes the script neater and easier to debug. Below is an example of the triangulation using the Sales.CurrencyRate table in the AdventureWorks2012 database.\nAs always, if you have any feedback and can suggest a simpler way of performing the triangulation I would love to hear it.\nThis post was originally published at Adatis on the 12th of May 2015.\n","permalink":"http://localhost:1313/posts/sql/triangulation-arbitrage/","summary":"\u003cp\u003eSome systems use a single currency as a base, which is something that I noticed recently when working with IBM Cognos Controller, e.g. USD to convert local currencies into. But what if you want / need to rebase into another currency but still retain the original base?\u003c/p\u003e\n\u003cp\u003eThis doesn\u0026rsquo;t appear to be easy to achieve within Cognos Controller itself, but it is achievable within SQL and a wider ETL framework.\u003c/p\u003e","title":"Currency Conversion in SQL using Triangulation Arbitrage"},{"content":"","permalink":"http://localhost:1313/posts/strategy/data-company/","summary":"","title":""}]